\section{PDF Parsing}
\label{sec:parsing}

%%%%% Trust Chain subsection %%%%
\input{trustchain} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parsing PDF: The details}
\label{sec:parsingfile}

We described the physical structure of a PDF file in \Cref{sec:pdf},
but the processing sequence may not be so apparent.
The following paragraphs describe the processing necessary to correctly parse a PDF file containing incremental updates.

PDF parsing begins by locating the PDF Header, as it is not uncommon for PDF files to have 
preamble bytes (such as an HTTP header, HTML or XML). The offset to the start of the PDF file 
(PDF offset zero)
within a physical file can then be determined from the \lstcd{\%} sign in \lstcd{\%PDF-x.y}. 
Processing continues by seeking to end-of-file and locating the last end-of-file marker \lstcd{\%\%EOF} in the physical file.

It is then necessary to locate the \emph{last} \lstcd{startxref} keyword and the following PDF file byte offset 
to the last cross-reference table in the PDF. Note also that this requires parsing \emph{backwards}
which is unnatural for many programming languages and is a proven source of parser differentials. 
This PDF byte offset must also be adjusted to a physical file offset by accounting for any preamble bytes prior to the PDF Header.

The parsing sublanguage depends on the form of the incremental update, with
conventional cross-reference tables being simpler and largely independent of
other processing. Cross-reference streams however are more complex as they are
often compressed and thus require the pre-DOM parser to ``trust'' the stream
extent dictionary data.

The parser must then locate either the \lstcd{xref} keyword for
conventional PDF cross-reference tables, or a cross-reference stream, identified by tokens of the form \lstcd{x y obj} . 
A parser must then determine if the PDF object is a
semantically valid cross-reference stream by further parsing the stream extent dictionary and 
validating the necessary key/value pairs and also recognizing the \lstcd{stream} keyword after the dictionary end token \lstcd{>>} and the \lstcd{endstream} keyword (see \cref{fig:XRefStm}). 

In the case of conventional PDF
cross-reference tables, after the cross reference table will be the
trailer dictionary, identified by the \lstcd{trailer} keyword. 
Note that this algorithm is at
odds with the file structure as defined in the PDF standard: the Trailer section is formally defined
to contain the trailer dictionary and \lstcd{startxref} keyword, yet the parsing algorithm
requires locating the first trailer \emph{after} the end of the cross-reference table 
(versus the last trailer dictionary above the \lstcd{startxref} keyword). 
Alternatively for PDF 1.5 and later files with cross-reference
streams, the trailer dictionary data will be in the stream extent
dictionary of the cross reference stream. 

Any previous cross-reference data is identified by the value of the \lstcd{/Prev} entry in either
the trailer dictionary or the stream extent dictionary of a cross-reference stream. The
value of the \lstcd{/Prev} key the byte offset to the immediately
preceding cross-reference data which, again, can either be a conventional
cross-reference table and to the start of the \lstcd{xref} keyword, or to a
cross-reference stream. This process repeats, working from the most recent incremental
update back through time to the original PDF document.

In each incremental update, the trailer dictionary is required to duplicate all keys from the previous
trailer and update accordingly. Of particular note is the \lstcd{/Size} entry, which must be
one greater than the largest object number allocated in the PDF file. Objects with numbers greater
than \lstcd{/Size} are defined to be the special PDF \lstcd{null} object.

Data in each cross reference table must be parsed to
identify the byte offset to the start of each PDF object, whether this be a file offset to an indirect object in a Body section, or a relative object position within an object stream (and where the object position is transformed to a byte offset within the object stream from the first line of text in the object stream). Note also that with conventional PDF cross-reference tables there is no definition for the byte offset to the end of an object, however for cross-reference streams this can be pre-determined.

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Specifying a Parser, not Specifying a Validator}
\label{sec:spec-approach}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.50\linewidth]{figures/validator.pdf}
    \caption{Validator}
    \label{fig:validator}
    
    \includegraphics[width=0.50\linewidth]{figures/parser.pdf}
    \caption{Parser}
    \label{fig:parser}
\end{figure}

A \emph{key} idea in ``SafeDocs'' is that we are trying to understand and
capture extant---used in the wild---PDF parsers.  Creating an implementation
that perfectly matches the PDF specification is not particularly useful
and would only parse a small fraction of extant PDF files.

See \Cref{fig:validator} where we show a validator (or a format \emph{recognizer}),
its function is to produce a DOM when we have a valid PDF, it must fail
otherwise.  Compare this to figure \Cref{fig:parser} where we show a parser
which has a {\bf{completely different requirement}}: to efficiently construct
the correct DOM when given an valid PDF.  A \emph{parser} is willing, if not
happy, to accept files beyond the pale of PDF.
In the \emph{parser} diagram,
we use a cloud to suggest that for various parser tools,
each is going to accept a different subset due to
\begin{itemize}
\item redundancies in the format allow for different choices,
\item the tools do not interpret the Standard uniformly,
\item tools may traverse and evaluate implicit data structures differently, and
\item tools differing in how they allow for ``minor'' errors or do error recovery.
\end{itemize}
Our goal for a \emph{parser specification} is to encompass any reasonable and
correct ``cloud''.  This goal is somewhat subjective, but generally this will
imply that we attempt to capture the laziness of various tools that only parse
or validate values upon demand.

% 
% A useful format specification must resolve a fundamental conflict between pr% ecision and restrictiveness.
% %
% An overly permissive specification, specifically of the PDF pre-DOM,  would % permit multiple compliant processors to produce radically different results,%  and thus would provide little ultimate assurance to format users.
% %
% Conversely, a specification that formalized all aspects of the standard rela% ted to pre-DOM components would prohibit almost all practical document proce% ssors, which almost never need to fully validate a document.

However, at the same time, it would be very desirable to
write a single piece of code from which we could extract
both a \emph{parser} as well as a \emph{validator}.
% our solution:
To resolve this conflict, we have %
\textbf{(1)} specified an implementation to be as \emph{lazy} as possible, in the sense that it minimizes the data read and validated whenever possible; and %
\textbf{(2)} extended this specification
with separate \lstcd{validate} predicates that, when executed,
would extend our implementation to form a complete validator.

%
% Because no implementations validate documents fully, and some implementations
% are surprisingly lazy, we want our specification to be very lazy.
% %
% Due to various redundancies in PDF, there is no one exact
% laziest semantics, but we attempt to create something reasonable.
% %
% The lazier we make our spec (while remaining correct),

One of our objectives is to test implementations
with respect to our parser specification;
we would test by validating that when an implementation produces a DOM,
that DOM is equivalent to the DOM produced by the specification.
This is why we want our specification to be lazier than most implementations.

% % introduce formal specification:
% We say that a specification is \emph{formal} if it is expressed in a languag% e whose semantics is amenable to a mechanizable definition.
% %
% By their nature, such definitions can offer precision of meaning that is a s% ignificant departure from standards expressed purely in prose;
% %
% in particular, they can provide a basic assurance that they do not rely on u% ndefined terms.
% %
% While their existence does not immediately solve all of the above issues in % format design, they are valuable artifacts for clarifying
% standards, understanding vulnerabilities, and aiding implementors.






