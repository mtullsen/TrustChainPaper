% A literate Haskell file
\section{Specifying the DOM's Foundations \note{4pp}}
\label{sec:specifying}

\subsection{A Specification?}
% REMEMBER: [terms: complies with standard, compatible with]
\note{motivate and explain our approach}

\begin{lstlisting}[style=meta]
- Lack of formality in standard. Thus, implementations:
  - are more effort
  - over implement, under implement, wrongly implement
  - backwards and forwards compatibility
  - "backwards parsing"
  - some requirements will not be checked by PDF readers
    ("writer only" file requirements) 
  - patch existing vs implement from scratch
- No definition of acceptable, reasonable error recovery
- Less than ideal design that reflects 27 years of an evolving standard
- Pre-DOM processing
  - is where many parsing errors & recovery occur
  - is non-trivial
  - involves multiple interdependent features and subtle dialects
  - involves multiple redundant features
    - schizophrenic if these features aren't mutually consistent
\end{lstlisting}

\begin{lstlisting}[style=meta]
- an implementation
  - should comply with the standard
  - should safely support less than standard
  - pragmatically support some common extant data malformations
    so as to be compatible with extant data
  - should carefully support more than the standard
  - should not "inf. loop"
  - lots of opportunities - failure to notice digitally signed PDFs
    that have been tampered!
    - where failure leads to "parser differential" without user
      warning (e.g. excessive trailer /Size)
    - PDF requires "backwards parsing" which is unnatural for
      many programming languages
      - elaborate?
\end{lstlisting}

In what follows we use Haskell \cite{Haskell} as a specification language for
our specification.
%
Our goal is to bridge the gap between the lowest level parsers
(parsing integers, parsing xref entries, etc.) and the processing
that happens after the DOM is created.   Thus, the primitive parsers
are omitted from this spec as well as a few other simple functions.
See \cref{sec:appendix1} for the Haskell type signatures for
the parsing primitives and other omitted functions.
The full spec can be found online at \todo{where?}.

\begin{itemize}
\item Our specification is formal and executable\footnote{
  Executable does not imply efficient, the specification is written
  primarily to be \emph{clear}.}.
  %
  This is our motivation for choosing Haskell over pseudo-code, English prose,
  and non-executable formalisms.  For the reader without a reading knowledge of
  Haskell, we understand that parts of the specification could be a little
  obscure, but our hope is that a precise, formal specification may prove to be
  more useful than pseudo-code or the like!
  
\item Our specification is purely functional: no ad hoc global variables are
  hidden, the data-dependencies in the spec fully represent all the
  data-dependencies.  Motivated by the Trust Chain issues
  (\cref{sec:trust-chain}), capturing all dependencies is a primary objective
  here.
  
\item Our specification hides no difficulties: one could implement a PDF parser
  by writing the omitted functions, but the spec stands complete, as is!
\end{itemize}

% The PDF "trust chain": higher levels of abstraction depend upon lower levels.
% These structures are not necessarily concrete values--e.g. parsed xref
% table--but they do exist `conceptually'.

\subsection{Pre-DOM Phases \note{PW's text, TODO: merge into below}}

{\bf{Phase 1}}

We find the PDF header \lstcd{\%PDF-x.y} (near start of the physical file, to
account for preamble), then the end of the PDF file \lstcd{\%\%EOF} (near the
end of the physical file), then "backwards parse" to find the last
\lstcd{startxref} keyword followed by an end-of-line sequence and an integer
value encoded as a sequence of ASCII bytes representing the byte offset in the
PDF file (which is then adjusted for any preamble to a physical byte offset),
and then locate either the \lstcd{xref} keyword for traditional PDF
cross-reference tables, or a PDF object that should be a cross-reference
stream. In the case of traditional PDF cross-reference tables, after the cross
reference table will be the trailer dictionary identified by the \lstcd{trailer}
keyword or, alternatively for PDF 1.5 and later files with cross-reference
streams, the trailer dictionary keys will be in the stream extent dictionary of
the cross reference stream. Of particular note is the \lstcd{Size} entry, which
is one greater than the largest object number allocated in the PDF file.

{\bf{Phase 2: we find and parse any incremental updates}}

These are identified by a \lstcd{Prev} entry in either the trailer
dictionary or the stream extent dictionary of a cross-reference stream. The
value of the \lstcd{Prev} key is another byte offset to the immediately
preceding incremental update which, again, can either be a traditional
cross-reference table and to the start of the \lstcd{xref} keyword, or to a
cross-reference stream. This process repeats, working from the most recent
update back through time to the original PDF document.

{\bf{Phase 3: ... we find and parse any incremental updates}}
  
Phase 3: data in each cross reference table must then be parsed to
identify the byte offset to the start of each PDF object. Note also that PDF
does not define the byte offset to the end of an object. There are two sets of
objects in every PDF document: the in-use list of PDF objects and a free list
of PDF objects. Object zero is always the start of the free list as it is not
otherwise a valid object number. For incremental updates, PDF object numbering
does not have to be sequential, with skipped object numbers assumed to be on
the free list (although this is not stated explicitly in the PDF
specification). Parsing depends on the form of the incremental update, with
traditional cross-reference tables being simpler and larger independent of
other processing. Cross-reference streams however are more complex as they are
usually compressed and thus require the pre-DOM parser to "trust" the stream
extent dictionary data.

{\bf{Phase 4: ...}}
  
Phase 4: using information from Phases 1, 2 and 3, the final set of
objects that comprise the final PDF document can be established. Each
incremental update can add new objects, mark existing in-use objects as free,
or reinstate previously freed objects. \pwnote{do we want to mention the
complexity of dig-sig here? what about cavities?}
  
\subsection{Pre-DOM phases}

%%%% Hs code not in paper %%%%
\iffalse
\begin{code}
{-# LANGUAGE EmptyDataDecls, TypeOperators, LambdaCase #-}
module Spec where

import           Control.Monad
import           Data.Char
import           Data.Foldable(foldlM)
import qualified Data.Map as M
import           Data.Map(Map)

import           Types
import           Utils
import           Primitives
\end{code}
\fi

\todo{MT: turn old bullet points/comments below into prose!}

What follows is the definition of \lstcd{pPDFDOM}; relying on lower level
parsers, it creates the \lstcd{DOM}.
\begin{code}
pPDFDom :: P DOM
\end{code}
The above line is a type signature, it is saying that
\lstcd{pPDFDom} is a ``monadic parser'' \lstcd{P} that returns a
value of type \lstcd{DOM}.
In Haskell a monad could contain many effectful constructs: global variables,
and etc. However, \lstcd{P} is a simple monad that effectively has one
read-only variable (the PDF file being read), and one mutable variable,
the offset in the file where reading (parsing) occurs.  I.e., we have
this primitive monadic function\footnote{
  We'll use the term ``action'' to refer to a monadic function:
  i.e., a function having a type scheme of ``\lstcd{a ->  P b}''.
}
\begin{codeNoExecute}
setInputAt :: Offset -> P ()
\end{codeNoExecute}
that can change where reading (parsing) takes place in the file.

Continuing with the definition of \lstcd{pPDFDom}:
\begin{code}
pPDFDom =
    do
    -- find '%PDF-x.y' at start of file, skipping up to 1K bytes:
    (version,headerOffset) <- findPDFHeader
    -- search backwards from EOF for 'startxref', gives up after 1000 bytes:
    (startxrefOff,xrefOff) <- findStartxrefThenParseToEOF
\end{code}

\haskellnote{The \lstcd{do} indicates this is monadic code.}
The action \lstcd{findPDFHeader} has type \lstcd{P ((Int,Int),Offset)};
it can fail if the header cannot be found or is malformed.
%
Note also that the above two function calls have no data dependencies, they
could be done in either order.

A non-obvious feature of PDF is that file offsets are in relation to, not the
beginning of the file, but the beginning of the \emph{PDF header}.
%
The next line abstracts over this problem once and for all:
\begin{code}  
    let jmp n = setInputAt (headerOffset+n)
\end{code}
We will need to pass \lstcd{jmp} to any actions that need to change
the offset in the file.

\begin{code}
    jmp xrefOff
    (xrefRaw, xrefEndOff) <- pXrefRaw :: P (XRefRaw,Offset)
    validate $
      verifyXrefRaw xrefRaw
        -- - this ensures no duplicate objectIds
        -- - we might parse and validate xref entries at this point
    jmp xrefEndOff
       -- This jmp is needed because pXrefRaw doesn't need to read
       -- the contents of each xref subsection, so let's leave the
       -- current file read location after the end.
\end{code}

For the sake of lucidity, we sometimes add \emph{type annotations} in the code,
note line 2 of the above: \lstcd{::P(XRefRaw,Offset)} indicates the type of
the expression preceding it.
%
Note \lstcd{validate} in line 3, this is a special function that we apply to
semantic checks (line 4) that are not necessary to create the DOM but which
could detect invalid or inconsistent PDFs.

For this initial XRef table---even without parsing xref subsections---we
know the list of object ids and we can find the xref entry for each object id.

\todo{add discourse on strict vs NCBUR/de-facto implementations ...}
\begin{lstlisting}[style=meta]
 - [maybe some of this covered in previous section]
 - enforcing full standard compliance with 20 byte (only) xref entries.
    - currently 19,21 byte xref entries are considered NCBUR!
 - if we were to allow 19-21 byte xref entries, we'd need
   to parse a lot more strictly and sooner.
 - nothing essential would change in our spec
\end{lstlisting}
Now to parsing more of the PDF trailer,
\begin{code}
    pSimpleWhiteSpace >> keyword "trailer"
    trailerDict <- pDictionary
    validate $
      do
      cs <- readTo startxrefOff -- get bytes up to `startxrefOff`
      return (all isSpace cs)
    trailerDict' <- dictToTrailerDict trailerDict

    let mPrev = trailerDict_getPrev trailerDict' :: Maybe Offset
        etc = trailerDict_Extras trailerDict'    :: Dict
          -- etc is a list of unknown key-value pairs
          -- we could be even lazier to allow
          --  - Xref and DOM analysis even when errors in dictionary
\end{code}

\begin{lstlisting}[style=meta]
 - generally we aren't checking dictionary keys, but ...
 - pSimpleWhiteSpace - comments aren't allowed
   [refactor to distinguish validate / NBCUR?]
 - if we don't do 'validate', we have cavities!
\end{lstlisting}
\begin{code}
    updates' <- pAllUpdates mPrev :: P [(XRefRaw, TrailerDict)]
       -- we've followed the 'Prev's and for each we
       --   - pXrefRaw     -- parse xref subsections (at raw level)
       --   - pTrailerDict -- similar to above, but
       --                     only reads/validates Prev key
\end{code}

\begin{lstlisting}[style=meta]
at this point
 - we have
   - parsed/validated minimally
   - rejected *some* invalid PDFs
   - no PDF 'values' parsed except trailer dictionaries
 - we can (without further 'parsing' or reading of input)
   - output trailer dictionaries
   - output high level info wrt incremental updates
 - we've detected
   - overlapping ObjIds in an xref table (and ...?)
 - we have NOT
   - parsed anything inessential to creating DOM
   - parsed the contents of xref entries
\end{lstlisting}

\begin{code}  
    let updates = (xrefRaw,trailerDict) : updates'
    dom <- makeDOMFromUpdates jmp updates

    -- N.B.: only when we have parsed everything, do we
    -- actually know the PDF version, because the version
    -- may be embedded in the DOM:
    version' <- updateVersionFromCatalogDict dom version
    if version' > (2,0) then
      warn "PDF file has version greater than 2.0"
    else
      -- version' <= (2,0)
      when (not (null etc)) $
        warn "trailer dictionary has unknown keys (per PDF 2.0)"
    return dom
\end{code}

The \lstcd{pDOM} function definition is now finished, but most of
the work is in the \lstcd{makeDOMFromUpdates} function,
where we create the DOM from the list of incremental updates:

\begin{code}
makeDOMFromUpdates :: (Offset -> P ()) -> [(XRefRaw, TrailerDict)] -> P DOM
makeDOMFromUpdates jmp updates =
    do
    -- combine all the updates to get a single map to offsets:
    xrefs <- combineAllXrefTables updates
             :: P (Map ObjId (Offset :+: Type2Ref))

    -- at this point
    --  - we know ALL the object ids in PDF

    -- parse all uncompressed objects (but leave streams undecoded):
    domPass1 <- mapM
                  (mMapLeft (\o-> do {jmp o; pTopLevelDef_UnDecStm}))
                  xrefs
                :: P (Map ObjId (TopLevelDef_UnDecStm :+: Type2Ref))
\end{code}
% FIXME: don't call the results of pass 1 'domPass1'

\todo{explain :+:}


\begin{lstlisting}[style=meta]
we are only NOW able to
  - verify/read toplevel stream data
    - b/c now indirect /Length and ... is defined in domPass1
    - decode ObjStm streams (if 1.5+)
\end{lstlisting}

\begin{code}
    -- decode streams into ByteStrings, also pre-processes ObjStm streams
    domPass2 <- mapM
                  (mMapLeft (extractStreamData domPass1))
                  domPass1
                :: P (Map ObjId (TopLevelDef :+: Type2Ref))
\end{code}

\begin{lstlisting}[style=meta]
at this point
 - can compute body cavities
 - ObjStm's have been pre-processed
   - but objects inside them not parsed
\end{lstlisting}

\begin{code}
    domFinal <- mapM
                 (return `either` derefType2Ref domPass2)
                  domPass2
                :: P (Map ObjId TopLevelDef)
    return domFinal
\end{code}

\begin{lstlisting}[style=meta]
at this point
 - every object referenced via xref has been parsed
 - However,
   - extraneous object defs in body are never parsed
   - unreferenced objects (per xref) in ObjStm's are never parsed
 - we positively know the PDF version (only now)
   - catalog dictionary might have been in an ObjStm
   - Q. is this intentional? this precludes lots of checks.
\end{lstlisting}

\subsection{Details of Streams and Type 2 References}

\todo{really want such detailed code?  put in Appendix?}

\begin{code}
-- | extractStreamData - since we now know all Lengths:
--   - 1st, with the file offset, read into a bytestring
--   - 2nd, if an ObjStm, decodes/validates the stream
--     - NOTE: this processing done just once, not each time
--       that we "index" into the ObjStm.
extractStreamData ::
     Map ObjId (TopLevelDef' Offset :+: a)
  -> TopLevelDef' Offset
  -> P (TopLevelDef' ByteString)
extractStreamData _dom' (TLD_ObjStm _)    = error "unexpeced ObjStm"
extractStreamData _dom' (TLD_Value v)     = return $ TLD_Value v
extractStreamData dom' (TLD_Stream d off) =
  do
  len  <- getKeyOfType "Length" T_Int d  -- indirect OR direct
  len' <- derefValue dom' len            -- now an integer direct
  decodingParameters
       <- extractDecodingParameters d
  bs   <- decodeStream len' decodingParameters off
  return $ TLD_Stream d bs

derefType2Ref ::
     Map ObjId (TopLevelDef :+: Type2Ref)
  -> Type2Ref
  -> P TopLevelDef
derefType2Ref dom' (Type2Ref oi j) =
  do
  tld       <- derefTLD dom' (oi,0)
  ObjStm ss <- getObjStm tld      -- make sure the object is ObjStm
  s         <- case safeIndex ss j of
                 Just s  -> return s
                 Nothing -> error "bad type2 ref: index out of bounds"
  v <- parseString pValue s
       -- note that streams cannot be inside ObjStm
  return $ TLD_Value v

getObjStm :: TopLevelDef' ByteString -> P ObjStm
getObjStm (TLD_ObjStm x) = return x
getObjStm _              = error "expected ObjStm"
\end{code}
   
\subsection{Can we Avoid Multiple Phases?}
\label{sec:single-pass-problems}
  
\todo{compare the above with a monolithic approach that APPEARS
  to not need multiple phases.}

\subsection{Incremental Updates: Further Details}
   
\begin{code}
-- | combineAllXrefTables updates - 
--   - for each update
--     - parses each xref subsection into a list of xref entries
--   - merges all the xref tables into a single mapping
--     - when no errors/inconsistencies
combineAllXrefTables
  :: [(XRefRaw, TrailerDict)] -> P (Map ObjId (Offset :+: Type2Ref))
combineAllXrefTables updates =
  do
  updates' <- mapM pUpdate updates  
  indices' <- mapM (createIndex . fst) updates' 
  index    <- foldlM mergeIndices M.empty indices'
  return index
\end{code}

\begin{lstlisting}[style=meta]
 - we've lost information:
   - which update an object is part of
   - object history
   - object definitions that are no longer reachable
 - fails on
   - malformed xref entries
   - mixture of xref table and xref streams [PW?]
 - should detect (or fail) on
   - trailer dicts that aren't consistent between updates
   - incremental updates that are "weird/nonsensical"
     - free-ing dead objects
     - unconventional use of generation numbers
 - IF updates are defined by xref STREAMS
   - no problem: as we can fully parse xref stream (w/ dict) as
     there is no dependence of xref STREAMS on DOM
     - NOTE: clarificaton to PDF working group regarding this.
   - we'll have Type2Ref's in addition to Offset's
      
 - NOTE 
   - when the latter, the ObjectId -> Offset must be available
       - in current or previous (or next!) xref stream
         - BTW, pervasive design issue: must partial updates be valid?
\end{lstlisting}

\subsection{Incremental Updates and Signatures: What A Tangled Web}
           
\todo{Need to mention that validating a PDF with a digital signature entails
  identifying at which iteration of the PDF document the dig-sig was applied and
  then validating the dig-sig in the context of that specific DOM and the
  objects that were in effect at that instance in time.}
\mtnote{Yes, but maybe elsewhere. Note: digital signatures \emph{break} the
  update abstraction [the abstraction that renderers can ignore the xrefs and
    updates and need only access the DOM]
}



