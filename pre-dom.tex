% A literate Haskell file

\section{Specifying the DOM's Foundations \note{4pp}}
\label{sec:specifying}

\subsection{Specifications}
% REMEMBER: [terms: complies with standard, compatible with]

What do we want from a PDF implementation?
Among other things we want it to
\begin{itemize}
\item comply with the standard,
\item when not supporting full standard, do so gracefully,
\item support common extant data malformations
  to be compatible with extant data, without introducing ambiguities
  or causing other unintended consequences.
\end{itemize}
At the same time, it should be resilient against all attacks.

The job of an implementor is challenging due to many factors:
\begin{itemize}
\item The intrinsic complexity of PDF:
  PDF is a \emph{less than ideal} design that reflects 27 years of
  an evolving standard.
  PDF has multiple redundant features.
  PDF is architected to allow for efficient implementations of
  large files (``efficiency hacks'').
  PDF involves multiple sublanguages and embedded formats.
  PDF contains complex parsing needs:
  E.g., searching backwards, embedded file offsets.
\item Lack of formality in standard. Thus, implementations
  require more effort to comply.
  Implementations commonly over implement, under implement,
  and wrongly implement the standard.
  Clearly, writing a PDF implementation from scratch is challenge,
  implementors are incentivized to patch existing code (correct or not).
\item The standard defines one thing: \emph{What is a valid PDF?},
  and leaves other decisions to the implementation:
  (1) What should absolutely \emph{not} be allowed (because in the real world
    implementations are more or less relaxed)? And similarly,
    what are deemed to be acceptable, reasonable error recovery methods?
  (2) What is required to support backwards and forwards compatibility?
  (3) What is done when redundant features are inconsistent: which, or
    neither, has priority?
    Similarly, what is done when the stated PDF version and the PDF
    constructs used don't match?
  (4) What is \emph{required} from the PDF writer versus
    what do we \emph{require} the PDF reader to check?
  (5) If given an incorrect PDF, is the behavior undefined\footnote{
      I.e., like ``undefined'' in the C programming language sense: the
      compiler can do anything it wants!
    } or \emph{must} we detect and report this?
\end{itemize}
\mtnote{this last, (5), seems pretty key and
  feels like a major omission in the standard! If the answer is
  yes, then there's nothing keeping an implementation from only
  doing enough to ``work on good pdfs'' without even checking that it
  IS a good PDF.  Thus, we have the tool that only reads the 'e'
  in ``endobj''.  Is this reasonable?  Are there vulnerabilities that
  would exist in such implementations?
  }
Failure to faithfully implement the standard can result in ambiguities
between implementations as well as direct vulnerabilities (such as
Shadow Attacks or DOS attacks).

Writing a \emph{formal specification} is not going to solve all the above,
but we believe it is a strong first step toward clarify the
standard, understanding the vulnerabilties, and aiding the PDF implementor.

We use Haskell \cite{Haskell} as a specification language for
our specification.
%
The scope of our specification is the gap between the low
level parsers (parsing integers, parsing XRef entries, etc.) and the
processing that happens after the DOM is created (stages 5 \& 6).
%
Thus, the primitive parsers are assumed, not included in this spec,
other formalisms\footnote{Such as daedalus, see \todo{}
} would be better suited to specifying the parsers.
\mtnote{need?: Also, for the sake of focusing on the tricky parts and ignoring
  the tedious (not that it is always obvious which is which),
  we do not show code for many of the simple, tedious parts.
}
See \cref{sec:appendix1} for the Haskell type signatures for the
parsing primitives and various other functions.
%
The full spec can be found online at \cite{daedalusrepo}.

\begin{itemize}
\item Our specification is formal and executable\footnote{
  Executable does not imply efficient, the specification is written
  primarily to be \emph{clear}.}.
  %
  This is our motivation for choosing Haskell over pseudo-code,
  English prose, and non-executable formalisms.  For the reader
  without a reading knowledge of Haskell, we understand that parts of
  the specification could be a little obscure, but our hope is that a
  precise, formal specification may prove to be more useful than
  pseudo-code or the like!
  
\item Our specification is purely functional: no ad hoc global variables are
  hidden, the data-dependencies in the spec fully represent all the
  data-dependencies.  Motivated by the Trust Chain issues
  (\cref{sec:trust-chain}), our objective was to capture all dependencies.
  
\item Our specification hides no difficulties: one could implement a PDF parser
  by writing the omitted functions, but the spec stands complete, as
  is\footnote{One caveat: the addition of support for \emph{some} features
  could require some re-design.  E.g., signature validation, as discussed in
  \cref{sec:updates-and-signatures} would require non-trivial changes.
  }!
\end{itemize}

% need to say: These structures are not necessarily concrete values--e.g. parsed
% XRef table--but they do exist `conceptually'.

This spec supports PDF 2.0, including compressed objects and XRef streams.
%
It (safely) ignores linearization data, and in hybrid XRef PDFs
it ignores the traditional xrefs for pre 1.5 readers.
It processes signatures (as incremental updates) but it does not support
validation of signatures.

\mtnote{brings up issues:
  How much validation to do?
  Are we specifying a renderer or a text-extractor?
  Are we supporting the validation of signatures (which radically
  changes things)?}

\subsection{Stages 1 - 4, Preliminaries}

%%%% begin: Hs code not in paper %%%%
\iffalse
\begin{code}
{-# LANGUAGE EmptyDataDecls, TypeOperators, LambdaCase #-}
module Spec where
import           Control.Monad
import           Data.Char
import           Data.Foldable(foldlM)
import qualified Data.Map as M
import           Data.Map(Map)
import           Types
import           Utils
import           Primitives
import           Streams
\end{code}
\fi
%%%% end: Hs code not in paper %%%%

What follows is the definition of \lstcd{pPDFDOM} which does the
parsing \& computation for stages 1-4;
it creates the \lstcd{DOM} relying on primitive parsers.
\begin{code}
pPDFDom :: P DOM
\end{code}
The above line is a type signature, it is saying that
\lstcd{pPDFDom} is a ``monadic parser'' \lstcd{P} that returns a
value of type \lstcd{DOM}.
In Haskell a monad can sequence many effectful constructs: global variables,
and etc. However, \lstcd{P} is a simple monad that effectively has one
read-only variable (the PDF file being read), and one mutable variable,
the offset in the file where reading (parsing) occurs.  I.e., we have
a single effectful primitive monadic function\footnote{
  We'll use the term ``action'' to refer to a monadic function:
  i.e., a function having a type scheme of ``\lstcd{a -> P b}''.
}:
\begin{codeNoExecute}
seekPrimitive :: Offset -> P () -- move read pointer to offset in file.
\end{codeNoExecute}

\subsection{Stage 1: Find \& Parse Header and Trailer}

Now to define \lstcd{pPDFDom}:
\begin{code}
pPDFDom =
    do
    -- find '%PDF-x.y' at start of file, searching the first 1000 bytes:
    (version, headerOffset) <- findPDFHeader
    -- search backwards from EOF for 'startxref', gives up after 1000 bytes:
    (startxrefOff, xrefOff) <- findStartxrefThenParseToEOF
\end{code}

\haskellnote{The \lstcd{do} keyword indicates the start of monadic code.}
The action \lstcd{findPDFHeader} has type \lstcd{P ((Int,Int),Offset)};
it can fail if the header cannot be found or is malformed.
%
Note also that the above two function calls have no data dependencies, they
could be done in either order.

A non-obvious feature of PDF is that file offsets are in relation to, not the
beginning of the file, but the beginning of the \emph{PDF header}.
%
The next line abstracts over this problem once and for all:
\begin{code}  
    let seek n = seekPrimitive (headerOffset+n)
\end{code}
We will need to pass \lstcd{seek} to any actions that need to change
the offset in the file.

The code for \lstcd{findStartxrefThenParseToEOF} is
not elucidated here as it is more tedious than instructive.
In English,
\begin{quote}
Find the ``EOF marker'' \lstcd{\%\%EOF} (near the end of the physical
file), then "Backwards parse" to find the last \lstcd{startxref}
keyword followed by an end-of-line sequence and an integer value
encoded as a sequence of ASCII bytes representing the byte offset in
the PDF file (which is then adjusted for any preamble to a physical
byte offset), and then locate either the \lstcd{xref} keyword for
traditional PDF cross-reference tables, or a PDF object that should be
a cross-reference stream.  In the case of traditional PDF
cross-reference tables, after the cross reference table will be the
trailer dictionary identified by the \lstcd{trailer} keyword or,
alternatively for PDF 1.5 and later files with cross-reference
streams, the trailer dictionary keys will be in the stream extent
dictionary of the cross reference stream.
\end{quote}
% TODO: reduce indentation

\subsection{Stage 2: Find \& Parse Incremental Updates}

\lstset{numbers=right}
\begin{code}
    seek xrefOff
    (xrefRaw, xrefEndOff) <- pXrefRaw :: P (XRefRaw,Offset)
    validate $
      verifyXrefRaw xrefRaw
        -- - this ensures no duplicate objectIds
        -- - we might parse and validate XRef entries at this point
    seek xrefEndOff
       -- This seek is needed because pXrefRaw doesn't need to read
       -- the contents of each XRef subsection, so let's leave the
       -- current file read location after the end.
\end{code}
\lstset{numbers=none}

For the sake of lucidity, we sometimes add \emph{type annotations} in
the code, note in line 2 of the above that \lstcd{::P(XRefRaw,Offset)}
indicates the type of the expression preceding it.
%
Note \lstcd{validate} in line 3, this is a special function that we apply to
semantic checks (line 4) that are not necessary to create the DOM but which
could detect invalid or inconsistent PDFs.

For this initial XRef table---even without parsing XRef subsections---we
know the list of object ids and we can find the XRef entry for each object id.

\begin{lstlisting}[style=meta]
 - [maybe some of this covered in previous section]
 - enforcing full standard compliance with 20 byte (only) XRef entries.
    - currently 19,21 byte XRef entries are considered NCBUR!
 - if we were to allow 19-21 byte XRef entries, we'd need
   to parse a lot more strictly and sooner.
 - nothing essential would change in our spec
\end{lstlisting}

\mttodo{this is traditional XRef specific, update!}
  
Now to parsing more of the PDF trailer
\begin{code}
    pSimpleWhiteSpace -- no comments allowed between XRef table and 
    keyword "trailer"
    trailerDict <- pDictionary
    validateAction $
      -- ensures nothing in the cavity between dictionary and ``startxref''
      do
      cs <- readToPrimitive startxrefOff -- get bytes up to `startxrefOff`
      return (all isSpace cs)

    trailerDict' <- dictToTrailerDict trailerDict
    let mPrev = trailerDict_getPrev trailerDict' :: Maybe Offset
        etc = trailerDict_Extras trailerDict'    :: Dict
          -- etc is a list of unknown key-value pairs
\end{code}

Note \lstcd{validateAction}: it differs from \lstcd{validate} in that
its argument can have side-effects (fail or change the file reading point).

\begin{lstlisting}[style=meta]
 - generally we aren't checking dictionary keys, but ...
 - pSimpleWhiteSpace - comments aren't allowed
 - if we don't do 'validate', we have cavities!
\end{lstlisting}

\mttodo{we have now parsed the trailer but ...}

\begin{code}
    updates' <- pAllUpdates mPrev :: P [(XRefRaw, TrailerDict)]
       -- we've followed the 'Prev's and for each we
       --   - pXrefRaw     -- parse XRef subsections (at raw level)
       --   - pTrailerDict -- similar to above, but
       --                     only reads/validates Prev key
    let updates = (xrefRaw,trailerDict) : updates'
\end{code}

\mttodo{integrate this text:}
... these are identified by a \lstcd{Prev} entry in either the trailer
dictionary or the stream extent dictionary of a cross-reference stream. The
value of the \lstcd{Prev} key is another byte offset to the immediately
preceding incremental update which, again, can either be a traditional
cross-reference table and to the start of the \lstcd{xref} keyword, or to a
cross-reference stream. This process repeats, working from the most recent
update back through time to the original PDF document.

\begin{lstlisting}[style=meta]
at this point
 - we know ALL the object ids in PDF
 - we have
   - parsed/validated minimally
   - rejected *some* invalid PDFs
   - no PDF 'values' parsed except trailer dictionaries
 - we can (without further 'parsing' or reading of input)
   - output trailer dictionaries
   - output high level info wrt incremental updates
 - we've detected
   - overlapping ObjIds in an XRef table (and ...?)
 - we have NOT
   - parsed anything inessential to creating DOM
   - parsed the contents of XRef entries
\end{lstlisting}

Now we finish the definition of \lstcd{pDOM}, making calls
to the actions for stage 3 and stage 4.

\begin{code}  
    -- Stage 3: combine all the updates to get a single map to offsets
    xrefs <- combineAllXrefTables updates
             :: P (Map ObjId (Offset :+: Type2Ref))

    -- Stage 4:
    dom <- transformXRefMapToObjectMap seek xrefs
    
    -- Miscellanea:
    version' <- updateVersionFromCatalogDict dom version
    if version' > (2,0) then
      warn "PDF file has version greater than 2.0"
    else
      -- version' <= (2,0)
      when (not (null etc)) $
        warn "trailer dictionary has unknown keys (per PDF 2.0)"
    validate $
      versionAndDomConsistent version' dom
    return dom
\end{code}

Due to PDF-1.5 additions, we are now in the odd position that we
cannot determine the PDF version until after we have created the DOM.
So although we
can check that the created \lstcd{dom} is consistent with
\lstcd{version'},
% 
we cannot use \lstcd{version'} to validate any of the other processing
in stages 1-4.
%
For instance, one would like to verify that Object Streams are not
used when the version is PDF-1.4 or earlier.
%
Such a check would be possible if we were to update the spec to pass
more information through the stages so we could verify more after
computation of the DOM.

In the next two sections we will look at the definitions of
\lstcd{combineAllXrefTables} (stage 3) and
\lstcd{transformXRefMapToObjectMap} (stage 4).

\subsection{Stage 3: Merge Incremental Updates}

Of particular note is the \lstcd{Size} entry, which
is one greater than the largest object number allocated in the PDF
file.
\mttodo{for last; either: implement enough to expose this or add verbiage}

\mttodo{integrate PW's text:}
In this stage, data in each cross reference table must then be parsed to
identify the byte offset to the start of each PDF object. Note also that PDF
does not define the byte offset to the end of an object. There are two sets of
objects in every PDF document: the in-use list of PDF objects and a free list
of PDF objects. Object zero is always the start of the free list as it is not
otherwise a valid object number. For incremental updates, PDF object numbering
does not have to be sequential, with skipped object numbers assumed to be on
the free list (although this is not stated explicitly in the PDF
specification). Parsing depends on the form of the incremental update, with
traditional cross-reference tables being simpler and larger independent of
other processing. Cross-reference streams however are more complex as they are
usually compressed and thus require the pre-DOM parser to "trust" the stream
extent dictionary data.

Each incremental update can add new objects, mark existing in-use objects as
free, or reinstate previously freed objects.
\mttodo{address the above in spec: either add to spec OR indicate that
  we are not ``refining'' this.}

\begin{code}
-- | combineAllXrefTables updates - 
--   - for each update
--     - parses each XRef subsection into a list of XRef entries
--   - merges all the XRef tables into a single mapping
--     - when no errors/inconsistencies
combineAllXrefTables
  :: [(XRefRaw, TrailerDict)] -> P (Map ObjId (Offset :+: Type2Ref))
combineAllXrefTables updates =
  do
  updates' <- mapM pUpdate updates  
  indices' <- mapM (createIndex . fst) updates' 
  index    <- foldlM mergeIndices M.empty indices'
  return index
\end{code}

\begin{lstlisting}[style=meta]
 - we've lost information:
   - which update an object is part of
   - object history
   - object definitions that are no longer reachable
 - fails on
   - malformed XRef entries
   - mixture of XRef table and XRef streams [PW?]
 - should detect (or fail) on
   - trailer dicts that aren't consistent between updates
   - incremental updates that are "weird/nonsensical"
     - free-ing dead objects
     - unconventional use of generation numbers
 - IF updates are defined by XRef STREAMS
   - no problem: as we can fully parse XRef stream (w/ dict) as
     there is no dependence of XRef STREAMS on DOM
     - NOTE: clarificaton to PDF working group regarding this.
   - we'll have Type2Ref's in addition to Offset's
      
 - NOTE 
   - when the latter, the ObjectId -> Offset must be available
       - in current or previous (or next!) XRef stream
         - BTW, pervasive design issue: must partial updates be valid?
\end{lstlisting}

\pwnote{and, depending on attack mode, referencing "dead objects" via a later
incremental update}

\subsection{Stage 4: Transform XRef Map to Object Map (DOM)}

Getting this stage right was a key motivation for writing our
specification, and it turned out to be surprisingly complicated.
\begin{code}
transformXRefMapToObjectMap
  :: (Offset -> P ()) -> Map ObjId (Offset :+: Type2Ref) -> P DOM
transformXRefMapToObjectMap seek xrefs0 = do
\end{code}
It has three separate stages all of which do some reading/parsing of
the file. Stage 4.1
transforms \lstcd{xrefs0} into \lstcd{xrefs1}, note the types:
\begin{codeNoExecute}
  xrefs0 :: Map ObjId (Offset               :+: Type2Ref) 
  xrefs1 :: Map ObjId (TopLevelDef_UnDecStm :+: Type2Ref)
\end{codeNoExecute}
\haskellnote{
We are using the infix type operator \lstcd{a :+: b} as a synonym for
\lstcd{Either a b}, \lstcd{Either} is the Haskell sum type having two constructors
\lstcd{Left} and \lstcd{Right}.
}
%
We look up traditional \lstcd{XRef}'s and parses the objects,
see line 3
\begin{code}
    -- Stage 4.1: parse all uncompressed objects
    xrefs1 <- mapM
                (mMapLeft (\o-> do {seek o; pTopLevelDef_UnDecStm}))
                xrefs0
\end{code}
We can't do more in stage 4.1 because we can't yet decode
object streams nor can we decode streams: in both cases these top
level objects \emph{cannot} always be parsed without looking up
integers in the DOM: as \lstcd{Length} keys and similar might contain
indirect references to top level integers.
\keypoint{
  In PDF we need to lookup and parse some objects in the DOM
  before we can parse other objects in the DOM!}
% TODO: ^ rewrite to make clearer.

Stage 4.2 further refines the xref map, computing the second from the first
\begin{codeNoExecute}
  xrefs1 :: Map ObjId (TopLevelDef_UnDecStm :+: Type2Ref)
  xrefs2 :: Map ObjId (TopLevelDef          :+: Type2Ref) 
\end{codeNoExecute}
in this code
\begin{code}
    -- Stage 4.2: decode stream bytes, pre-process ObjStm streams
    xrefs2 <- mapM
                (mMapLeft (extractStreamData xrefs1))
                xrefs1
\end{code}
We have a sufficiently complete \lstcd{DOM} in \lstcd{xrefs1} in which
we can lookup the integers that are needed to finish decoding the
streams we left undecoded.
If we cannot find the referenced integer in the DOM, we have a true PDF
error.  Any indirect \lstcd{Length} values are required to be at the
top level.

At this point, in \lstcd{xrefs2}, we still have \lstcd{ObjId}'s that point
(via \lstcd{Type2Ref}) into \lstcd{ObjStm} streams.
Only in stage 4.2 were we able to decode the stream inside of
which is the object to be parsed.
% this text is rather complicated/tedious!

\mttodo{bring in some of the Alg. Data Type defs?}

So at this point, (1) we can compute body cavities \todo{define this} as
we can parse all the top level objects, (2) object streams have been
pre-processed, but we have not yet parsed the objects inside them.

Finally, stage 4.3 computes the second from the first
\begin{codeNoExecute}
  xrefs2       :: Map ObjId (TopLevelDef :+: Type2Ref) 
  domCandidate :: Map ObjId TopLevelDef
\end{codeNoExecute}
using the same pattern we've been seeing but now we no longer have 
the sum, we have a mapping from \lstcd{ObjId}'s to PDF Values:
\begin{code}
    -- Stage 4.3: resolve Type 2 references
    domCandidate <- mapM
                     (return `either` derefType2Ref xrefs2)
                     xrefs2
    return domCandidate
\end{code}

At this point every object referenced via the XRef table has been
correctly parsed. However, (1) object definitions in the PDF body
area that are not ``XRef referenced'' are neither read nor parsed,
% 
(2) objects inside an \lstcd{ObjStm} stream that are not ``XRef
referenced'' are also neither read nor parsed.

Note that the \emph{Catalog Dictionary} (which has the optional
\lstcd{Version} key) may have been in an Object stream, so in general
we may not know till this point which version of PDF we are
parsing\footnote{We wonder if this was the intention of the Standard!}.

\subsection{A One Stage Version?}
\label{sec:single-pass-problems}

\newcommand{\ssp}{$S$}
\newcommand{\dsp}{$D$}

One might ask if one might implement the above as a single stage?
Or another question, can we rewrite the specification to be a single stage?
The answer is \emph{Yes, but at a cost we don't want to pay!}
Let's refer to the above multi-stage specification as the \ssp{}
(staged) specification.
%
We will compare it to a single-stage specification, the \dsp{}
(dynamic) spec in which stage 1 is the same but stages 2, 3, and 4 are
merged together.
%
In our experience, PDF tools generally work more like \dsp{} than \ssp{}.

\dsp must be understood imperatively; here's a sketch of a possible
implementation in C-ish notation: \mttodo{put into figure; fixup lst rendering.}
\begin{myc}
  // xref and dom start with nothing in them.
  // - their size would in reality be dynamically allocated.
  XREF_ENTRY xref[Size];     
  DOM_ENTRY dom[Size];
  
  // the DOM is updated dynamically, on demand, via the following
  PdfValue deref(ObjId oi) {
    if (evald(dom[oi]))
      return dom[oi];
    else if (infiniteloopdetected())
      quit ();      
    else {
      o = getOffsetFromXref(xref, oi); // updates xref[oi]
      seek(o);
      v = parseObject;
      dom[oi] = v;
      return v;
    }
    
  parseObject {
    /* ... */ ; len = deref(oi'); /* ... */
  }
  
  OFFSET getOffsetFromXref(xref,oi){
    if xref_evald(xref[oi]) then
      return xref[oi];
    else {
      // follow Prev pointers if not in top xref
      //  - make sure no infinite loop in chasing Prev's
      /* ... */
      xref[oi] = offset;
      return xref[oi];
    }
  }
\end{myc}
The key things to note about the pseudo-code are
\begin{itemize}
\item \lstcd{XREF_ENTRY} and \lstcd{DOM_ENTRY} are both types that mutate progressively
   from unparsed/unknown to fully evaluated.
\item \lstcd{deref()} and \lstcd{parseObject()} are mutually recursive functions.
\item implementing \lstcd{infiniteLoopDetected()} is non-trivial.
\end{itemize}
Also to note about \dsp{}:
\begin{itemize}
\item it is \emph{not} equivalent to \ssp{}: \dsp{} potentially reads
  less of the input file and accepts more input files.  it is
  naturally ``lax'': it would for instance allow a \lstcd{Length} to
  be stored in an \lstcd{ObjStm} stream as long as an infinite loop
  wasn't detected.  However, we could extend (and complicate) \dsp{}
  to approximate \ssp{} better.
  % E.g.,
  %  - have a =derefLength= / =derefFromUncompressed=
  %  - More complicated than just this, because this won't catch error if we
  %    luck out and when we request the length it is already decoded.
\item it is nicely lazy if the PDF tool doesn't need to \lstcd{deref}
  every object identifiers, even more lazy than \ssp{}.
\end{itemize}

We believe that \ssp{} is clearly preferable to \dsp{} for these two
simple reasons:
\begin{itemize}
\item It corresponds to the standard, and does so obviously.
\item It does not use general recursion, so it is obvious that this
  algorithm terminates on all inputs.
\end{itemize}
While \ssp{} also gives us a few other advantages over \dsp{}:
\begin{enumerate}
\item The functional, declarative, and typed structure enables
  us to understand the stages conceptually.  (Even if one chooses
  not to implement in a like manner.)
\item It demonstrates the non obvious fact that one can
  implement stages 2-4 and keep the ``state of evaluation'' of each
  object identifier the same in each pass.
\item We know exactly what is and isn't parsed, regardless of the
  order in which one traverses the \lstcd{xref}.
\item It is intrisically parallelizable due to the extensive use of
  map-like combinators.
\item The declarative nature of \ssp{} makes it very amenable to
  modification: e.g., the simple addition of the \lstcd{validate}
  operator.
\end{enumerate}

\subsection{Incremental Updates and Signatures: What A Tangled Web \note{0.3pp}}
\label{sec:updates-and-signatures}

\todo{validating a PDF with a digital signature entails
  identifying at which iteration of the PDF document the dig-sig was applied and
  then validating the dig-sig in the context of that specific DOM and the
  objects that were in effect at that instance in time.}

\todo{digital signatures \emph{break} the
  update abstraction [the abstraction that renderers can ignore the XRefs and
    updates and need only access the DOM]
}

\subsection{Assessments}

Currently this specification adheres to the PDF 2.0 standard but this
limits its use, as so many PDF tools allow multiple variances from the
standard.
%
However, the opportunity is for us to use our specification to encode
some of these common extensions and to explore if the combination of
these extensions are truly unambiguous.

With our specification based approach, it would not be onerous
to refactor our specification to support different PDF variations:
\begin{itemize}
\item strict PDF-2.0.
\item strict PDF-2.0, validating everything.
\item PDF-2.0 with some common extensions that are determined to be unambiguous.
\end{itemize}

The conciseness of our specification is due to a few factors
\begin{enumerate}
\item our intention to make it as clear as possible,
\item ignoring some ``engineering'' aspects (e.g., error
   messages, recovery, etc.)
\item leaving out code for some of the simple, tedious functions.
\end{enumerate}
One line of future work would be to turn the specification into a
\emph{reference implementation} by the addition of
(1) code for the current, missing ``tedious parts'',
(2) links to implementations of the primitive parsers, and
(3) output of the DOM.

Future work might also involve further validation checks such as these:
(1) processing linearization data and ensuring consistency with the
non-linearized DOM;
(2) processing both ``branches'' of a hybrid PDF and ensuring
some form of constency between pre 1.5 readers and 1.5+ readers;
(3) adding signature validation, see \cref{sec:updates-and-signatures}.
