% A literate Haskell file

\section{Specifying the DOM's Foundations \note{4pp}}
\label{sec:specifying}

\subsection{Specifications}
% REMEMBER: [terms: complies with standard, compatible with]

What do we want from a PDF implementation?
Among other things we want it to
\begin{itemize}
\item comply with the standard,
\item when not supporting full standard, do so gracefully,
\item support common extant data malformations
  to be compatible with extant data, without introducing ambiguities
  or causing other unintended consequences.
\end{itemize}
At the same time, it should be resilient against all attacks.

The job of an implementor is challenging due to many factors:
\begin{itemize}
\item The intrinsic complexity of PDF:
  PDF is a \emph{less than ideal} design that reflects 27 years of
  an evolving standard
  PDF has multiple redundant features.
  PDF is architected to allow for efficient implementations of
  large files (``efficiency hacks'').
  PDF involves multiple sublanguages and embedded formats.
  PDF contains features beyond standard parsing and data-format
  tools: E.g, search backwards, embedded file offsets.
\item Lack of formality in standard. Thus, implementations
  require more effort to comply.
  Implementations commonly over implement, under implement,
  and wrongly implement the standard.
  Clearly, writing a PDF implementation from scratch is challenge,
  implementors are likely to patch existing code (correct or not).
\item The standard defines one thing: \emph{What is a valid PDF?},
  And it leaves other decisions to the implementation:
  (1) What should absolutely \emph{not} be allowed (because in the real world
    implementations are more or less relaxed)? And similarly,
    what is deemed to be acceptable, reasonable error recovery methods?
  (2) What is required to support backwards and forwards compatibility?
  (3) What to do when redundant features don't line up: Which, if
    either, has priority?
    Similarly, what to do when the stated PDF version and the PDF
    constructs used don't match?
  (4) What is \emph{required} from the PDF writer versus
     what do we \emph{require} the PDF reader to check?
\end{itemize}
Failure to faithfully implement the standard can result in ambiguities
between implemetations as well as direct vulverabilities (such as
Shadow Attacks or DOS attacks).

Writing a \emph{formal specification} is not going to solve all the above,
but we believe it is a strong first step toward understanding the
Standard, clarifying the vulnerabilties, and aiding PDF implementors.

In what follows we use Haskell \cite{Haskell} as a specification language for
our specification.
%
The scope of our specification is the gap between the low
level parsers (parsing integers, parsing XRef entries, etc.) and the
processing that happens after the DOM is created (stages 5 \& 6).
%
Thus, the primitive parsers are not included in this spec (they are
just assumed, other formalisms are better suited to specifying the
parsers).
\mtnote{need?: Also, for the sake of focusing on the tricky parts and ignoring
  the tedious (not that it is always obvious which is which),
  we do not show code for many of the simple, tedious parts.
}
See \cref{sec:appendix1} for the Haskell type signatures for the
parsing primitives and various other functions.
%
The full spec can be found online at \todo{where? in daedalus repo?}.

\begin{itemize}
\item Our specification is formal and executable\footnote{
  Executable does not imply efficient, the specification is written
  primarily to be \emph{clear}.}.
  %
  This is our motivation for choosing Haskell over pseudo-code,
  English prose, and non-executable formalisms.  For the reader
  without a reading knowledge of Haskell, we understand that parts of
  the specification could be a little obscure, but our hope is that a
  precise, formal specification may prove to be more useful than
  pseudo-code or the like!
  
\item Our specification is purely functional: no ad hoc global variables are
  hidden, the data-dependencies in the spec fully represent all the
  data-dependencies.  Motivated by the Trust Chain issues
  (\cref{sec:trust-chain}), our objective was to capture all dependencies.
  
\item Our specification hides no difficulties: one could implement a PDF parser
  by writing the omitted functions, but the spec stands complete, as
  is\footnote{One caveat: the addition of support for \emph{some} features
  could require some re-design.  E.g., signature validation, as discussed in
  \cref{sec:updates-and-signatures} would require non-trivial changes.
  }!
\end{itemize}

% need to say: These structures are not necessarily concrete values--e.g. parsed
% XRef table--but they do exist `conceptually'.

This spec supports PDF 2.0, including compressed objects and XRef streams.
%
It (safely) ignores linearization data, and in hybrid XRef PDFs
it ignores the pre 1.5 parts of the file.
It processes signatures (as incremental updates) but it does not support
validation of signatures.

\mtnote{brings up issues:
  How much validation to do?
  Are we specifying a renderer or a text-extractor?
  Are we supporting the validation of signatures (which radically
  changes things)?}

\subsection{Stages 1 - 4, Preliminaries}

%%%% begin: Hs code not in paper %%%%
\iffalse
\begin{code}
{-# LANGUAGE EmptyDataDecls, TypeOperators, LambdaCase #-}
module Spec where
import           Control.Monad
import           Data.Char
import           Data.Foldable(foldlM)
import qualified Data.Map as M
import           Data.Map(Map)
import           Types
import           Utils
import           Primitives
\end{code}
\fi
%%%% end: Hs code not in paper %%%%

What follows is the definition of \lstcd{pPDFDOM} which does the
parsing \& computation for stages 1-4;
it creates the \lstcd{DOM} relying on primitive parsers.
\begin{code}
pPDFDom :: P DOM
\end{code}
The above line is a type signature, it is saying that
\lstcd{pPDFDom} is a ``monadic parser'' \lstcd{P} that returns a
value of type \lstcd{DOM}.
In Haskell a monad can sequence many effectful constructs: global variables,
and etc. However, \lstcd{P} is a simple monad that effectively has one
read-only variable (the PDF file being read), and one mutable variable,
the offset in the file where reading (parsing) occurs.  I.e., we have
a single effectful primitive monadic function\footnote{
  We'll use the term ``action'' to refer to a monadic function:
  i.e., a function having a type scheme of ``\lstcd{a -> P b}''.
}
\begin{codeNoExecute}
setInputAt :: Offset -> P ()
\end{codeNoExecute}
that will update where reading (parsing) takes place in the file.

\subsection{Stage 1: Find \& Parse Header and Trailer}

Now to define \lstcd{pPDFDom}:
\begin{code}
pPDFDom =
    do
    -- find '%PDF-x.y' at start of file, searching the first 1000 bytes:
    (version,headerOffset) <- findPDFHeader
    -- search backwards from EOF for 'startxref', gives up after 1000 bytes:
    (startxrefOff,xrefOff) <- findStartxrefThenParseToEOF
\end{code}

\haskellnote{The \lstcd{do} keyword indicates the start of monadic code.}
The action \lstcd{findPDFHeader} has type \lstcd{P ((Int,Int),Offset)};
it can fail if the header cannot be found or is malformed.
%
Note also that the above two function calls have no data dependencies, they
could be done in either order.

A non-obvious feature of PDF is that file offsets are in relation to, not the
beginning of the file, but the beginning of the \emph{PDF header}.
%
The next line abstracts over this problem once and for all:
\begin{code}  
    let jmp n = setInputAt (headerOffset+n)
\end{code}
We will need to pass \lstcd{jmp} to any actions that need to change
the offset in the file.

The code for \lstcd{findStartxrefThenParseToEOF} is
not elucidated here as it is more tedious than instructive.
In English,
\begin{quote}
Find the ``EOF marker'' \lstcd{\%\%EOF} (near the end of the physical
file), then "Backwards parse" to find the last \lstcd{startxref}
keyword followed by an end-of-line sequence and an integer value
encoded as a sequence of ASCII bytes representing the byte offset in
the PDF file (which is then adjusted for any preamble to a physical
byte offset), and then locate either the \lstcd{xref} keyword for
traditional PDF cross-reference tables, or a PDF object that should be
a cross-reference stream.  In the case of traditional PDF
cross-reference tables, after the cross reference table will be the
trailer dictionary identified by the \lstcd{trailer} keyword or,
alternatively for PDF 1.5 and later files with cross-reference
streams, the trailer dictionary keys will be in the stream extent
dictionary of the cross reference stream.
\end{quote}
% TODO: reduce indentation

\subsection{Stage 2: Find \& Parse Incremental Updates}

\lstset{numbers=right}
\begin{code}
    jmp xrefOff
    (xrefRaw, xrefEndOff) <- pXrefRaw :: P (XRefRaw,Offset)
    validate $
      verifyXrefRaw xrefRaw
        -- - this ensures no duplicate objectIds
        -- - we might parse and validate XRef entries at this point
    jmp xrefEndOff
       -- This jmp is needed because pXrefRaw doesn't need to read
       -- the contents of each XRef subsection, so let's leave the
       -- current file read location after the end.
\end{code}
\lstset{numbers=none}

For the sake of lucidity, we sometimes add \emph{type annotations} in
the code, note in line 2 of the above that \lstcd{::P(XRefRaw,Offset)}
indicates the type of the expression preceding it.
%
Note \lstcd{validate} in line 3, this is a special function that we apply to
semantic checks (line 4) that are not necessary to create the DOM but which
could detect invalid or inconsistent PDFs.

For this initial XRef table---even without parsing XRef subsections---we
know the list of object ids and we can find the XRef entry for each object id.

\begin{lstlisting}[style=meta]
 - [maybe some of this covered in previous section]
 - enforcing full standard compliance with 20 byte (only) XRef entries.
    - currently 19,21 byte XRef entries are considered NCBUR!
 - if we were to allow 19-21 byte XRef entries, we'd need
   to parse a lot more strictly and sooner.
 - nothing essential would change in our spec
\end{lstlisting}

\mttodo{this is traditional XRef specific, update!}
  
Now to parsing more of the PDF trailer
\begin{code}
    pSimpleWhiteSpace -- no comments allowed between XRef table and 
    keyword "trailer"
    trailerDict <- pDictionary
    validateAction $
      -- ensures nothing in the cavity between dictionary and ``startxref''
      do
      cs <- readTo startxrefOff -- get bytes up to `startxrefOff`
      return (all isSpace cs)

    trailerDict' <- dictToTrailerDict trailerDict
    let mPrev = trailerDict_getPrev trailerDict' :: Maybe Offset
        etc = trailerDict_Extras trailerDict'    :: Dict
          -- etc is a list of unknown key-value pairs
\end{code}

Note \lstcd{validateAction}: it differs from \lstcd{validate} in that
its argument can have side-effects (fail or change the file reading point).

\begin{lstlisting}[style=meta]
 - generally we aren't checking dictionary keys, but ...
 - pSimpleWhiteSpace - comments aren't allowed
 - if we don't do 'validate', we have cavities!
\end{lstlisting}

\mttodo{we have now parsed the trailer but ...}

\begin{code}
    updates' <- pAllUpdates mPrev :: P [(XRefRaw, TrailerDict)]
       -- we've followed the 'Prev's and for each we
       --   - pXrefRaw     -- parse XRef subsections (at raw level)
       --   - pTrailerDict -- similar to above, but
       --                     only reads/validates Prev key
    let updates = (xrefRaw,trailerDict) : updates'
\end{code}

\mttodo{integrate this text:}
... these are identified by a \lstcd{Prev} entry in either the trailer
dictionary or the stream extent dictionary of a cross-reference stream. The
value of the \lstcd{Prev} key is another byte offset to the immediately
preceding incremental update which, again, can either be a traditional
cross-reference table and to the start of the \lstcd{xref} keyword, or to a
cross-reference stream. This process repeats, working from the most recent
update back through time to the original PDF document.

\begin{lstlisting}[style=meta]
at this point
 - we know ALL the object ids in PDF
 - we have
   - parsed/validated minimally
   - rejected *some* invalid PDFs
   - no PDF 'values' parsed except trailer dictionaries
 - we can (without further 'parsing' or reading of input)
   - output trailer dictionaries
   - output high level info wrt incremental updates
 - we've detected
   - overlapping ObjIds in an XRef table (and ...?)
 - we have NOT
   - parsed anything inessential to creating DOM
   - parsed the contents of XRef entries
\end{lstlisting}

Now we finish the definition of \lstcd{pDOM}, making calls
to the actions for stage 3 and stage 4.

\begin{code}  
    -- Stage 3: combine all the updates to get a single map to offsets
    xrefs <- combineAllXrefTables updates
             :: P (Map ObjId (Offset :+: Type2Ref))

    -- Stage 4:
    dom <- transformXRefMapToObjectMap jmp xrefs
    
    -- Miscellanea:
    version' <- updateVersionFromCatalogDict dom version
    if version' > (2,0) then
      warn "PDF file has version greater than 2.0"
    else
      -- version' <= (2,0)
      when (not (null etc)) $
        warn "trailer dictionary has unknown keys (per PDF 2.0)"
    validate $
      versionAndDomConsistent version' dom
    return dom
\end{code}

Due to PDF-1.5 additions, we are now in the odd position that we
cannot determine the PDF version until after we have created the DOM.
So although we
can check that the created \lstcd{dom} is consistent with
\lstcd{version'},
% 
we cannot use \lstcd{version'} to validate any of the other processing
in stages 1-4.
%
For instance, one would like to verify that Object Streams are not
used when the version is PDF-1.4 or earlier.
%
Such a check would be possible if we were to update the spec to pass
more information through the stages so we could verify more after
computation of the DOM.

In the next two sections we will look at the definitions of
\lstcd{combineAllXrefTables} (stage 3) and
\lstcd{transformXRefMapToObjectMap} (stage 4).

\subsection{Stage 3: Merge Incremental Updates}

Of particular note is the \lstcd{Size} entry, which
is one greater than the largest object number allocated in the PDF
file.
\mttodo{for last; either: implement enough to expose this or add verbiage}

\mttodo{integrate PW's text:}
In this stage, data in each cross reference table must then be parsed to
identify the byte offset to the start of each PDF object. Note also that PDF
does not define the byte offset to the end of an object. There are two sets of
objects in every PDF document: the in-use list of PDF objects and a free list
of PDF objects. Object zero is always the start of the free list as it is not
otherwise a valid object number. For incremental updates, PDF object numbering
does not have to be sequential, with skipped object numbers assumed to be on
the free list (although this is not stated explicitly in the PDF
specification). Parsing depends on the form of the incremental update, with
traditional cross-reference tables being simpler and larger independent of
other processing. Cross-reference streams however are more complex as they are
usually compressed and thus require the pre-DOM parser to "trust" the stream
extent dictionary data.

Each incremental update can add new objects, mark existing in-use objects as
free, or reinstate previously freed objects.
\mttodo{address the above in spec: either add to spec OR indicate that
  we are not ``refining'' this.}

\begin{code}
-- | combineAllXrefTables updates - 
--   - for each update
--     - parses each XRef subsection into a list of XRef entries
--   - merges all the XRef tables into a single mapping
--     - when no errors/inconsistencies
combineAllXrefTables
  :: [(XRefRaw, TrailerDict)] -> P (Map ObjId (Offset :+: Type2Ref))
combineAllXrefTables updates =
  do
  updates' <- mapM pUpdate updates  
  indices' <- mapM (createIndex . fst) updates' 
  index    <- foldlM mergeIndices M.empty indices'
  return index
\end{code}

\begin{lstlisting}[style=meta]
 - we've lost information:
   - which update an object is part of
   - object history
   - object definitions that are no longer reachable
 - fails on
   - malformed XRef entries
   - mixture of XRef table and XRef streams [PW?]
 - should detect (or fail) on
   - trailer dicts that aren't consistent between updates
   - incremental updates that are "weird/nonsensical"
     - free-ing dead objects
     - unconventional use of generation numbers
 - IF updates are defined by XRef STREAMS
   - no problem: as we can fully parse XRef stream (w/ dict) as
     there is no dependence of XRef STREAMS on DOM
     - NOTE: clarificaton to PDF working group regarding this.
   - we'll have Type2Ref's in addition to Offset's
      
 - NOTE 
   - when the latter, the ObjectId -> Offset must be available
       - in current or previous (or next!) XRef stream
         - BTW, pervasive design issue: must partial updates be valid?
\end{lstlisting}

\pwnote{and, depending on attack mode, referencing "dead objects" via a later
incremental update}

\subsection{Stage 4: Transform XRef Map to Object Map (DOM)}

\begin{code}
transformXRefMapToObjectMap
  :: (Offset -> P ()) -> Map ObjId (Offset :+: Type2Ref) -> P DOM
transformXRefMapToObjectMap jmp xrefs =
    do
    -- Stage 4.1: parse all uncompressed objects (but leave streams undecoded):
    xrefs1 <- mapM
                (mMapLeft (\o-> do {jmp o; pTopLevelDef_UnDecStm}))
                xrefs
              :: P (Map ObjId (TopLevelDef_UnDecStm :+: Type2Ref))
\end{code}
% FIXME: don't call the results of pass 1 'domPass1'

\todo{explain :+:}

\begin{lstlisting}[style=meta]
we are only NOW able to
  - verify/read toplevel stream data
    - b/c now indirect /Length and ... is defined in domPass1
    - decode ObjStm streams (if 1.5+)
\end{lstlisting}

\begin{code}
    -- Stage 4.2: decode streams into ByteStrings, also pre-processes ObjStm streams
    xrefs2 <- mapM
                (mMapLeft (extractStreamData xrefs1))
                xrefs1
              :: P (Map ObjId (TopLevelDef :+: Type2Ref))
\end{code}

\begin{lstlisting}[style=meta]
at this point
 - can compute body cavities
 - ObjStm's have been pre-processed
   - but objects inside them not parsed
\end{lstlisting}

\begin{code}
    -- Stage 4.3: resolve Type 2 references
    domCandidate <- mapM
                     (return `either` derefType2Ref xrefs2)
                     xrefs2
                    :: P (Map ObjId TopLevelDef)
    return domCandidate
\end{code}

\begin{lstlisting}[style=meta]
at this point
 - every object referenced via XRef has been parsed
 - However,
   - extraneous object defs in body are never parsed
   - unreferenced objects (per XRef) in ObjStm's are never parsed
 - we positively know the PDF version (only now)
   - catalog dictionary might have been in an ObjStm
   - Q. is this intentional? this precludes lots of checks.
\end{lstlisting}

\subsection{Stage 4: Details of Streams and Type 2 References}
\mtnote{really want such detailed code?  put in Appendix?}

\begin{code}
-- | extractStreamData - since we now know all Lengths:
--   - 1st, with the file offset, read into a bytestring
--   - 2nd, if an ObjStm, decodes/validates the stream
--     - NOTE: this processing done just once, not each time
--       that we "index" into the ObjStm.
extractStreamData ::
     Map ObjId (TopLevelDef' Offset :+: a)
  -> TopLevelDef' Offset
  -> P (TopLevelDef' ByteString)
extractStreamData _dom' (TLD_ObjStm _)    = error "unexpeced ObjStm"
extractStreamData _dom' (TLD_Value v)     = return $ TLD_Value v
extractStreamData dom' (TLD_Stream d off) =
  do
  len  <- getKeyOfType "Length" T_Int d  -- indirect OR direct
  len' <- derefValue dom' len            -- now an integer direct
  decodingParameters
       <- extractDecodingParameters d
  bs   <- decodeStream len' decodingParameters off
  return $ TLD_Stream d bs

derefType2Ref ::
     Map ObjId (TopLevelDef :+: Type2Ref)
  -> Type2Ref
  -> P TopLevelDef
derefType2Ref dom' (Type2Ref oi j) =
  do
  tld       <- derefTLD dom' (oi,0)
  ObjStm ss <- getObjStm tld      -- make sure the object is ObjStm
  s         <- case safeIndex ss j of
                 Just s  -> return s
                 Nothing -> error "bad type2 ref: index out of bounds"
  v <- parseString pValue s
       -- note that streams cannot be inside ObjStm
  return $ TLD_Value v

getObjStm :: TopLevelDef' ByteString -> P ObjStm
getObjStm (TLD_ObjStm x) = return x
getObjStm _              = error "expected ObjStm" -- cannot recover from this
\end{code}
   
\subsection{Can We Avoid Multiple Stages? \note{0.3pp}}
\label{sec:single-pass-problems}

\newcommand{\ssp}{$S$}
\newcommand{\dsp}{$D$}

The short answer to ``Can we avoid multiple stages?'' is
``Yes, but at a cost we don't want to pay.''

Let's refer to the above multi-stage specification as the \ssp{}
(staged) specification.
%
We will compare it to a single-stage specification, the \dsp{}
(dynamic) spec in which stage 1 is the same but stages 2, 3, and 4 are
merged together.
In our experience, implementations seem to work more like \dsp{} than \ssp{}.

\mttodo{Here's how \dsp{} works: ...}
\begin{lstlisting}[style=meta]
  - you have =derefId= command
    - very lazy & you only access/read what is needed
    - it calls itself recursively!
      - TODO :: add check for infinite loop
    - e.g., if a "dependent on DOM parser" (stream with indirect), 
      then immediately look that up and parse that, then return
\end{lstlisting}

\mttodo{To compare \ssp{} and \dsp{} ...}
\begin{lstlisting}[style=meta]
[TOO MUCH DETAIL:]
- implementation /D/ (Dynamic)
  - NOTE, /D/ compared to implementation /N/     
    - it *IS* nicely lazy if you don't want to =derefId= all obj ids
      - doesn't parse unused ObjStms
    - more efficient than /N/ (?)
      - each object goes from unparsed to fully parsed
      - directly follows references without needing to recurse over ObjId Map
      - but ... every derefId needs to check evaled/not
    - con :: as currently done in pdf-hs-driver, allows bad PDFs
      - not detecting length in ObjStm unless *required*
      - we might have a recursive situation that is "well-defined"
      - help to have a =derefLength= / =derefFromUncompressed=
        - more complicated than just this, because this won't catch error if we
          luck out and the length is already decoded.
    - con :: no parallel execution, no parallel error messages
    - con :: imperative
    - con :: no way to create a validator from. ?
            
- reasons for /N/ over /D/
  - want to parse everything and be done
  - want to *efficiently* parse all objects
  - want to know (sooner) that all objects parse and pre-Dom works.
  - want to be assured that the code terminates 
  - elegance/simplicity in all objects being in same state of "evaluation"
\end{lstlisting}

\subsection{Incremental Updates and Signatures: What A Tangled Web \note{0.3pp}}
\label{sec:updates-and-signatures}

\todo{validating a PDF with a digital signature entails
  identifying at which iteration of the PDF document the dig-sig was applied and
  then validating the dig-sig in the context of that specific DOM and the
  objects that were in effect at that instance in time.}

\todo{digital signatures \emph{break} the
  update abstraction [the abstraction that renderers can ignore the XRefs and
    updates and need only access the DOM]
}

\subsection{Assessments}

Currently this specification adheres to the PDF-2.0 standard but this
may not be practical.
%
\todo{discourse on strict vs NCBUR/de-facto implementations ...}
Determining if common extensions are truly unambigous and extending
the spec \todo{... finish this thought.}

With our specification based approach, it would not be overly onerous
to refactor our specification to support three variations of PDF:
\begin{enumerate}
\item strict PDF-2.0
\item support common extensions that are unambiguous
\item validate everything
\end{enumerate}

The conciseness of the above specification is due to a few factors
\begin{enumerate}
\item an attempt to be as clear as possible
\item ignoring some ``engineering'' aspects (e.g., error
   messages, recovery, etc.)
\item leaving out code for some of the simple, tedious functions.
\end{enumerate}
One line of future work would be to make this specification complete,
in particular to implement the ``tedious parts'' and to link to
implementations of primitive parsers.

Future work could involve further validation checks
(1) processing Linearization data and ensuring consistency with the
non-linearized DOM;
(2) processing both ``branches'' of a hybrid PDF and ensuring
some form of constency between pre 1.5 readers and 1.5+ readers;
(3) adding signature validation, see \cref{sec:updates-and-signatures}.
