\section{Specifying the PDF DOM's Foundations}
\label{sec:specifying}

This section %
states our goals in producing a formal partial definition of PDF
(\Cref{sec:spec-goals}) and then presents the specification's core
definitions (\Cref{sec:core}), followed by its definitions of each
stage of the PDF Trust Chain (\Crefrange{sec:stage-1}{sec:stage-4}).
%
\Cref{sec:assessing} analyses the resulting specification.

\subsection{Specifying PDF: Goals and Approach}
\label{sec:spec-goals}

% REMEMBER terms: complies with standard, compatible with
Ideally, a PDF implementation should:
\begin{itemize}
\item comply with the PDF standard,
\item when not fully supporting the standard, do so gracefully,
\item support common PDF malformations
  to be compatible with extant PDFs, but \emph{without} introducing ambiguities
  or causing other unintended consequences (we refer to this as \emph{permissiveness}),
\item and all the while, contain minimal bugs, robustly cope with unexpected input and stay resilient
  against all attacks.
\end{itemize}
Achieving the above is challenging due to many factors:
\begin{itemize}
\item The intrinsic complexity of PDF:
  PDF is a \emph{less than ideal} design that reflects almost 30 years of
  an evolving standard.
  %
  PDF has multiple redundant features and is explicitly designed to facilitate efficient processing of
  large files (``efficiency hacks'').
  %
  PDF involves multiple sublanguages (dialects) and embedded formats.
  %
  PDF contains complex parsing needs, such as backwards searching and
  embedded file offsets.
\item Lack of formality in the standard. Thus, implementators
  need more effort to understand it and comply with its intentions.
  Implementations commonly over implement, under implement,
  and wrongly implement the standard.
  Because writing a rewriting  PDF implementation from scratch is a significant and costly effort,
  implementors are incentivized to patch existing code (correct or not).
\item The standard primarily defines just one thing: \emph{what is a valid PDF file},
  with only a few processor requirements. Many technical details are also delegated to the numerous normative references.
  This leaves the majority of decisions up to each implementation:
  (1) What should absolutely \emph{not} be allowed (because in the real world
    implementations are more or less relaxed)? And similarly,
    what are deemed to be acceptable, reasonable error recovery methods?
  (2) What is required to support backwards and forwards compatibility?
  (3) What is done when redundant features are inconsistent with one
    another: which, or neither, has priority?
    Similarly, what is done when the stated PDF version and the PDF
    constructs used don't match?
  (4) What is \emph{required} from the PDF writer versus
    what do we \emph{require} the PDF reader to check?
  (5) Behavior when given in incorrect PDF.
\end{itemize}

Failure to faithfully implement the standard can result in ambiguities
between implementations, parser differentials as well as direct vulnerabilities (such as
Shadow Attacks or DoS attacks).

\mtnote{this last, (5), seems pretty key and
  feels like a major omission in the standard! If the answer is
  yes, then there's nothing keeping an implementation from only
  doing enough to ``work on good pdfs'' without even checking that it
  IS a good PDF.  Thus, we have the tool that only reads the 'e'
  in ``endobj''.  Is this reasonable?  Are there vulnerabilities that
  would exist in such implementations?
}

% define formal specification:
A \emph{formal specification} of a format does not immediately resolve
all of the above issues, but it is a critical artifact for clarifying
the standard, understanding the vulnerabilties, and aiding
implementors of PDF processors.
%
In this work, we have produced a specification of one component of PDF
in the Haskell programming language~\cite{jones2003haskell}.
%
A detailed presentation of Haskell's features is beyond the scope of
this paper, so we note only that it is a statically typed, pure,
functional programming language with a lazy;
%
we review its other features throughout the paper when they are
relevant.

% say more about the spec's scope:
The scope of our specification is the gap between the low level
parsers (parsing integers, parsing XRef entries, etc.) and the
processing that happens after the DOM is created (stages 5 and 6).
%
I.e., stages 1--4.
%
Primitive parsers are assumed, not included in this spec, as other
formalisms, such as the \emph{DaeDaLus Data Description
  Language}~\cite{daedalusrepo}, are better suited to specifying the
primitive parsers.
%
See \cref{sec:appendix1} for a list of the primitive parsers
with their type signatures.
%
The full specification of DaeDaLus is publicly available
online~\cite{daedalusrepo}.

\begin{itemize}
\item Our specification is formal and executable (though not
  necessarily efficient).
  %
  We already have the PDF standard: which is not always clear, not always
  consistent. 
  This is our motivation for choosing Haskell over English prose or
  pseudo-code or other informal approaches.
  For the reader
  without a reading knowledge of Haskell, we understand that parts of
  the specification could be a little obscure, but our hope is that a
  precise, formal specification may prove to be more useful than
  pseudo-code or the like!
  
\item Our specification is purely functional: no ad hoc global variables are
  hidden, the data-dependencies in the spec fully represent all the
  data-dependencies.  Motivated by the Trust Chain issues
  (\cref{sec:trust-chain}), our objective was to capture all dependencies.
  
\item Our specification hides no difficulties: one could implement a PDF parser
  by writing the omitted functions, but the spec stands complete, as
  is!
  % \footnote{One caveat: the addition of support for \emph{some} features
  % could require some re-design.  E.g., signature validation, as discussed in
  % \cref{sec:updates-and-signatures} would require non-trivial changes.
  % }
\end{itemize}

This spec supports PDF 2.0, including compressed objects and XRef streams.
%
It (safely) ignores linearization data, and in hybrid XRef PDFs
it ignores the traditional \xref{} tables designed for pre PDF 1.5 readers.
It processes signatures (as incremental updates) but it does not support
validation of signatures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Core definitions}
\label{sec:core}
%%%% begin: Hs code not in paper %%%%
\iffalse
\begin{code}
{-# LANGUAGE EmptyDataDecls, TypeOperators, LambdaCase #-}
module Spec where
import           Control.Monad
import           Data.Char
import           Data.Foldable(foldlM)
import qualified Data.IntSet as IntSet
import           Data.List
import qualified Data.Map as M
import           Data.Map(Map)
import           Types
import           Utils
import           Primitives
import           Streams
\end{code}
\fi
%%%% end: Hs code not in paper %%%%

\lstcd{pPDFDom} (\cref{fig:spec}) implements stages 1 through 4 (illustrated in \cref{fig:pdf-trust-chain}).
% 
\lstcd{pPDFDom}'s type (line 1) denotes that it
is a \emph{monadic} parser \lstcd{P} that returns a value of type
\lstcd{DOM}.
%
In general, monads are a rich class of parameterized types that
describe a surprising variety of data and computations.
%
For the purposes of this paper, it suffices to view a monad as a container that provides an operation to sequence effectful \emph{actions} (specifically, functions from some type to a monad over a potentially distinct type) in a purely functional manner;
%
effects can be global variables (or mutable state), exceptions, I/O, and etc.

% do notation:
Haskell provides a syntactic form for concisely sequencing actions, structured as the keyword \lstcd{do} followed by bindings of the form \lstcd{x <- A}, which denotes that the action \lstcd{A} is performed and its resulting value is bound to variable \lstcd{x} to be used by further actions.
%
E.g.,
\begin{codeNoExecute}
  topAction :: P Int
  topAction = do
              result1 <- action1 args1...
              let x = <PURELY-FUNCTIONAL-EXPRESSION>
                  f x = <FUNCTION> -- not an action
              result2 <- action2 result1 args2...
              return (anyPureFunction result2)
\end{codeNoExecute}
%
\todo{do we really need this extra example? Can't we just point to Fig. 6?}

% walk through P:
The specific parser monad \lstcd{P} abstracts effects on a parser's state.
%
Its state includes %
\textbf{(1)} one read-only variable, which stores the PDF file being read, and %
\textbf{(2)} one mutable variable \rdloc{}, which stores the offset in the file at which the next byte is read.
%
\rdloc{} is accessed by primitive parsers (which have type \lstcd{P})
and is updated by sequencing the parsing action
\begin{codeNoExecute}
  seekPrimitive :: Offset -> P ()
  seekPrimitive off = <update readLocation with 'off'>
\end{codeNoExecute}
%
\mttodo{readLocation equals \rdloc{}?}
%
\texttt{P} parsers implicitly track parsing errors, which are thrown when an input document is not in the defined format.
%
I.e., each parsing action of the form \lstcd{x <- A} can be viewed as attempting to parse according to action \lstcd{A}, binding a result to variable \lstcd{x} only on success and failing otherwise.

% type annotations:
To aid understandability, expressions are occasionally annotated with types (see \cref{fig:spec} lines 8, 11, and 14).

\begin{figure}[t]
\centering
\lstset{numbers=right}
\begin{code}
pPDFDom :: P DOM
pPDFDom =
    do
    -- Stage 1:
    (seek,xrefOffset,version) <- findHeaderAndTrailer
    -- Stage 2:
    updates <- parseAllIncUpdates seek xrefOffset
               :: P [(XRefRaw, TrailerDict)]
    -- Stage 3: combine all the updates to get a single xref table
    xrefs <- combineUpdates seek updates
             :: P (Map ObjId (Offset :+: Type2Ref))
    -- Stage 4:
    dom <- transformXRefMapToObjectMap seek xrefs
           :: P DOM
    -- final checks and validations:
    finalValidations dom version updates
    return dom
\end{code}
\caption{A formalization of pre-DOM stages 1--4 in Haskell.}
\label{fig:spec}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stage 1: find and parse the header and trailer}
\label{sec:stage-1}

% give the code for stage 1:
Stage 1 is defined as follows:
\begin{code}
findHeaderAndTrailer :: P (SEEK, Offset, Version)
findHeaderAndTrailer =
    do
    -- find "%PDF-x.y" near start of file, searching the first 1000 bytes:
    (version, headerOffset) <- findPDFHeader 1000

    -- search backwards from EOF for 'startxref', gives up after 1001 bytes:
    xrefOff <- findStartxrefThenParseToEOF 1001
    
    let seek n = seekPrimitive (headerOffset+n)
    return (seek, xrefOff, version)
    
type SEEK = Offset -> P () -- type of seek
\end{code}

Neither of the magic numbers $1000$ or $1001$ used in the definition are specified precisely
by the PDF standard, which requires only that they be numeric values
that are ``reasonably small'' \todo{is this a quote from the standard? How did we pick 1000 and 1001?}.
%
This is an immediate cause of ambiguity in the standard (and thus an
invitation for \pd{} in conformant document processors).

File offsets in a PDF are not defined in relation to the beginning of
the file, as might be expected, but instead to the beginning of the
\emph{PDF header};
%
the \lstcd{seek} provides a clean abstraction that defines this
aspect.
%
But we will need to pass \lstcd{seek} to all the actions that need to
``seek'' in the PDF file (note this being done in \cref{fig:spec}).

Because the two function calls (lines 5 and 8) have no data
dependencies, they can be performed in any order.
%
\haskellnote{The spec has no global variables beyond \rdloc{} and neither function
  depends on it.}

The full definition of \lstcd{findStartxrefThenParseToEOF} is
omitted, to simplify the presentation.
%
In abstract terms, it:
\begin{enumerate}
\item Finds the ``EOF marker'' \lstcd{\%\%EOF} (near the end of the physical
  file);
\item Parses ``backwards" to find the last \lstcd{startxref} keyword
  followed by an end-of-line sequence;
\item Parses the integer value encoded as a sequence of ASCII bytes
  (this represents the byte offset in the PDF file, which as noted
  above is adjusted for any preamble to the Header).
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stage 2: Find and Parse Incremental Updates}
\label{sec:stage-2}
%
Stage 1 only finds the start of trailer;
%
the trailer dictionary itself, along with the complete cross-reference table, are defined in stage 2 as follows:
%
\lstset{numbers=right}
\begin{code}
type Update = (XRefRaw, TrailerDict)

pIncUpdate_Traditional :: SEEK -> P Update
pIncUpdate_Traditional seek =
    do
    (xrefRaw, xrefEndOff) <- pXrefRaw :: P (XRefRaw,Offset)
    validate \$
      verifyXrefRaw xrefRaw
        -- this ensures no duplicate objectIds
    seek xrefEndOff
       -- This seek is needed because pXrefRaw doesn't need to read
       -- the entries of each XRef subsection, so let's leave the
       -- current file read location after the xref table.

    pSimpleWhiteSpace -- no comments allowed between XRef table and ..
    keyword "trailer"
    trailerDict  <- pDictionary
    trailerDict' <- dictToTrailerDict trailerDict
                    -- ensures dictionary has proper keys
    return (xrefRaw, trailerDict')
\end{code}
\lstset{numbers=none}

\lstcd{validate} (line 7) is a special function used to demarcate semantic checks (line 8) that are not necessary to
create the DOM but which could detect invalid or inconsistent PDFs.
%
The check could be used in a ``validate'' mode, but would not necessarily be performed by all implementations.

% produces unparsed xref table subsections:
The resulting subsections of the \xref{} table are unstructured, each of type \lstcd{XRefRaw}, rather than a sequence of fully parsed \xref{} entries;
%
this aspect of the definition is revisited and fully motivated in \Cref{sec:stage-3}.
%
The partially structured representation still retains a set of \objid{}'s and for each, the offset of its of its \xref{} entry.
%
Constructing the representation critically relies on a precise requirement established in the standard: an \xref{} entry must have exactly 20 bytes.

The \xref{} table and trailer themselves are simply instances of incremental updates, which are parsed as follows:
\lstset{numbers=right}
\begin{code}
parseAllIncUpdates :: SEEK -> Offset -> P [Update]
parseAllIncUpdates seek offset =
  parseAllIncUpdates' IntSet.empty seek offset

parseAllIncUpdates' :: IntSet.IntSet -> SEEK -> Offset -> P [Update]
parseAllIncUpdates' prevSet seek offset =
    if offset `IntSet.member` prevSet then
      error "recursive incremental updates"
    else
      do
      seek offset
      (xref,trailerDict) <- pIncUpdate seek
      case trailerDict_getPrev trailerDict of   -- lookup 'Prev' key
        Nothing      -> -- no Prev key, we're done:
                        return [(xref,trailerDict)]
        Just offset' -> -- Prev key found, find updates starting at
                        -- offset':
                        do
                        us <- parseAllIncUpdates'
                                (IntSet.insert offset prevSet)
                                seek
                                offset'
                        return ((xref,trailerDict):us)
\end{code}
\lstset{numbers=none}
%
Much of the above definition is dedicated to ensuring that potential loops in the \lstcd{Prev} offsets do not cause the definition to lose well-foundedness (or in operational terms, that the natural corresponding parser does not loop infinitely).
%
The set \lstcd{prevSet} tracks the offsets of every update processed.
%
The definition %
\textbf{(1)} \lstcd{seek}s to the \lstcd{offset} and parses the update, then 
\textbf{(2)} checks for a \lstcd{Prev} key.
%
If no key is present, then parsing is complete;
%
otherwise, parsing continues with an extended set of offsets.

% supporting xref tables:
\lstcd{pIncUpdate} supports \xref{} tables as defined in versions of PDF up to 1.5 and represented using cross-reference streams:
\begin{code}
pIncUpdate :: SEEK -> P (XRefRaw,TrailerDict)
pIncUpdate seek =
      pIncUpdate_Traditional seek
  .|. pIncUpdate_XrefStream seek
      -- I.e., parse one or the other,
      -- syntactically, they are mutually exclusive.

pIncUpdate_XrefStream :: SEEK -> P (XRefRaw,TrailerDict)
pIncUpdate_XrefStream seek = notImplementedYet
\end{code}
%
The function \lstcd{pIncUpdate_XrefStream} is more
complex than \lstcd{pIncUpdate_Traditional} but the resulting
data is equivalent.
%
It also determines subsections without parsing the \xref{} entries.

% recap:
Upon success, stage 2 produces values from which a client can directly determine all \objids{} in the PDF and produce the trailer dictionaries and incremental updates.
%
The only PDF values that have been parsed are trailer dictionaries, but many documents can be rejected, including those with recurring \objids{} in the \xref{} tables.
\mtnote{check this phrasing}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stage 3: combine incremental updates}
\label{sec:stage-3}
%
In stage 3, updates are processed from last to first:
\begin{code}
combineUpdates :: SEEK
               -> [(XRefRaw, TrailerDict)] 
               -> P (Map ObjId (Offset :+: Type2Ref))
combineUpdates seek updates =
    do
    let (xref,dict):us = updates -- safe because: null(updates)==False

    -- parse all xref entries in last (near EOF) update:
    xrefEntries <- mapM (thawXRefEntry seek xref) (getObjIds xref)

    -- initial Map:
    let tbl0 :: Map ObjId XRefEntry
        tbl0 = M.fromList xrefEntries

    -- merge each update into tbl0:
    tbl1  <- foldlM (mergeUpdate seek) tbl0 us
    return (removeFrees tbl1)

type XRefEntry = Free :+: (Offset :+: Type2Ref)
\end{code}

Each incremental update can add new objects, mark existing objects in use as free, or update objects.
%
\lstcd{XRefEntry} is defined to process updates from last to first;
%
it could have been defined alternatively to parse \xref{} entries only as needed.
%
\todo{why did we choose the option that we did?}

\todo{say what mergeUpdate does, informally}

\begin{code}
mergeUpdate :: SEEK
            ->  (Map ObjId XRefEntry)
            -> (XRefRaw, TrailerDict)
            -> P (Map ObjId XRefEntry)
mergeUpdate seek map0 (xref,dict) =
    do
    let objIds       = getObjIds xref
        neededObjIds = objIds \\ M.keys map0
                       -- set subtraction

    -- only parse (thaw) needed XRef entries:
    newEntries <- forM neededObjIds
                       (thawXRefEntry seek xref)
    return
      (M.union map0 (M.fromList (newEntries :: [(ObjId,XRefEntry)] )))

data Free = Free  -- currently we are ignoring free objects
\end{code}
\lstcd{mergeUpdate} uses the functions
\begin{code}
removeFrees :: Map k XRefEntry -> Map k (Offset :+: Type2Ref)

thawXRefEntry :: SEEK -> XRefRaw -> ObjId -> P (ObjId, XRefEntry)

getObjIds :: XRefRaw -> [ObjId]
\end{code}
%
Their implementations are withheld, for simplicity.

For simplicity, the definition does not constraint \objids{} using the values bound to trailer dictionaries' \lstcd{Size} key;
%
the standard requires that:
\begin{itemize}
  \item Any object in a cross-reference section whose number is
    greater than the bound value shall be ignored and treated as missing.
  \item Equivalently, the value shall be greater by one than the highest object number defined in the PDF file.
  %
  \todo{how is this equivalent to the first item?}
\end{itemize}

Not only is this stage's definition complex, but there are multiple, apparently workable variations of \lstcd{mergeUpdate}, with distinct semantics:
\begin{itemize}
\item There appear to be multiple ways to enforce \lstcd{< Size} as we
  process individual updates.
\item When all objects are defined (we can determine this using
  \lstcd{Size}), we could stop processing previous updates.
\item There are many \lstcd{validate} instances we might add,
  some being unclear even what the standard would enforce:
  \begin{itemize}
  \item prohibit double frees
  \item prohibit the update of unfree objects
  \item prohibit updates that don't increment the generation number
  \item prohibit freeing of non-existent \objids{}.
  \item prohibit ``unconvential use'' of generation numbers.
  \item validate that the offset of new objects in the update are
    defined in the ``body region'' of the respective update (not point
    to previous nor subsequent regions)
  \end{itemize}
\end{itemize}
%
None of the above properties can be validated upon the completion of Stage 3.
%
Given that some of the properties may be indicators of shadow attacks or PDFs that are otherwise suspicious, Stage 3 is thus critically important as a stage for security validation.

% merging Stages 2 and 3:
While Stages 2 and 3 could feasibly be defined as a single stage, it would effectively force every compliant to parse and validate to a degree that may not be needed by many applications.
%
Our specification was designed to include as much validation as possible as instances of \lstcd{validate}.
%
\todo{wasn't clear how XRefRaw connects}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stage 4: Transform XRef Map to Object Map (DOM)}
\label{sec:stage-4}
%
The apparent complexity of stage 4 in particular was a primary motivation of our work.
%
Stage 4 is implemented as the following function:
\begin{code}
transformXRefMapToObjectMap
  :: SEEK -> Map ObjId (Offset :+: Type2Ref) -> P DOM
transformXRefMapToObjectMap seek xrefs0 = do
\end{code}
%
The stage contains three distinct substages, each of which directly parse the document's bytes.
%
Stage 4.1 transforms \lstcd{xrefs0} into \lstcd{xrefs1}, with types:
\begin{codeNoExecute}
  xrefs0 :: Map ObjId (Offset               :+: Type2Ref) 
  xrefs1 :: Map ObjId (TopLevelDef_UnDecStm :+: Type2Ref)
\end{codeNoExecute}
\haskellnote{
The type \lstcd{a :+: b} is a synonym for the Haskell sum type \lstcd{Either a b}, which has constructors
\lstcd{Left} and \lstcd{Right}.
}
%
Traditional \lstcd{XRef}'s are resolved and the resulting objects are parsed; see line 3:
\begin{code}
    -- Stage 4.1: parse all uncompressed objects
    xrefs1 <- mapM
                (mMapLeft (\o-> do {seek o; pTopLevelDef_UnDecStm}))
                xrefs0
\end{code}
Further parsing and validation is not possible in stage 4.1 because object streams, and streams in general, cannot yet be decoded: these top level objects cannot always be parsed without resolving \lstcd{Length} (and similar) keys, which may be bound to indirect references to integer values.

% stage 4.1:
Stage 4.2 further refines the \texttt{xref} map, computing the second from the first, with the following types:
\begin{codeNoExecute}
  xrefs1 :: Map ObjId (TopLevelDef_UnDecStm :+: Type2Ref)
  xrefs2 :: Map ObjId (TopLevelDef          :+: Type2Ref) 
\end{codeNoExecute}
\texttt{xrefs2} is defined as
\begin{code}
    -- Stage 4.2: decode stream bytes, pre-process ObjStm streams
    xrefs2 <- mapM
                (mMapLeft (extractStreamData xrefs1))
                xrefs1
\end{code}
%
\lstcd{xrefs1} is a sufficiently complete \lstcd{DOM} that it can be used to resolve integers needed to decode any currently undecoded streams.
%
If a claimed reference to an integer is not bound in the DOM, then the document is not a valid PDF: all indirect values bound to \lstcd{Length} keys must be at the document's top level.

At this point, in \lstcd{xrefs2}, we still have \lstcd{ObjId}'s that point
(via \lstcd{Type2Ref}) into \lstcd{ObjStm} streams.
%
Only in stage 4.2 were we able to decode the stream inside of
which is the object to be parsed.
% TODO this last text is rather complicated/tedious!

\mttodo{bring in some of the Alg. Data Type defs?}

Values computed in stage 4.2 are sufficient for computing document cavities.
%
Object streams are pre-processed, but any objects that they contain have not yet been parsed.

Finally, stage 4.3 computes \lstcd{domCandidate} from \lstcd{xrefs2}, with the following types:
\begin{codeNoExecute}
  xrefs2       :: Map ObjId (TopLevelDef :+: Type2Ref) 
  domCandidate :: Map ObjId TopLevelDef
\end{codeNoExecute}
\lstcd{domCandidate} is defined following a similar pattern to computation in previous stages.
%
However, unlike previous stages, sum types are no longer used; the result is a map from  \lstcd{ObjId}'s to PDF Values:
\begin{code}
    -- Stage 4.3: resolve Type 2 references
    domCandidate <- mapM
                     (return `either` derefType2Ref xrefs2)
                     xrefs2
    return domCandidate
\end{code}

At this point, every object referenced via the cross-reference table has been correctly parsed.
%
However, objects are not yet read or parsed if they are not referenced, whether they occur in the document's body or within an object stream.

The \emph{Catalog Dictionary} (which has the optional
\lstcd{Version} key) may have been in an Object stream, so in general the intended version of the document cannot be determined until this point;
%
it is arguably unclear if this was intended when defining the PDF standard.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Further Validation}
\label{sec:validating}

\mttodo{remove or fix the etc stuff}

We have a few checks and validations that we are unable to do until
the DOM is created:
\begin{code}  
finalValidations dom version updates =
    do    
    version' <- updateVersionFromCatalogDict dom version
    let etc = stub "extra keys in the trailer dict" :: Dict -- TODO!

    if version' > (2,0) then
      warn "PDF file has version greater than 2.0"
    else
      -- version' <= (2,0)
      when (not (null etc)) $
        warn "trailer dictionary has unknown keys (per PDF 2.0)"
    validate $
      versionAndDomConsistent version' dom
    
    validate $
      trailersConsistentAcrossUpdates updates
\end{code}

Due to PDF-1.5 additions, we are now in the odd position that we
cannot determine the PDF version until after we have created the DOM.
So although we
can check that the created \lstcd{dom} is consistent with
\lstcd{version'},
% 
we cannot use \lstcd{version'} to validate any of the other processing
in stages 1--4.
%
For instance, one would like to verify that Object Streams are not
used when the version is PDF-1.4 or earlier.
%
Such a check would be possible if we were to update the spec to pass
more information through the stages so we could verify more after
computation of the DOM.

\section{Assessing The Specification}
\label{sec:assessing}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Comparison with a definition consisting of only one stage}
\label{sec:single-pass-problems}

%%%%%%%%%%%%%%%
\begin{figure}[t]
  \centering
  \begin{myc}
    // xref and dom start with nothing in them:
    //  - in reality, their sizes would be dynamically allocated.
    XREF_ENTRY xref[Size];     
    DOM_ENTRY  dom[Size];
    
    // 'dom' is updated dynamically, on demand, via 'deref':
    PdfValue deref(ObjId oi) {
      if (evald(dom[oi]))
        return dom[oi];
      else if (infiniteloopdetected())
        quit ();      
      else {
        o = lookupOffsetInXref(oi); // updates xref[oi]
        seek(o);
        v = parseObject();
        dom[oi] = v;
        return v;
      }
      
    PdfValue parseObject (){
      ...
      if (some Stream object) {
        ...; len = deref(oi'); ... // need this to finish parsing!
      }
      ...
    }
    
    OFFSET lookupOffsetInXref(oi) {
      if xref_evald(xref[oi]) then
        return xref[oi];
      else {
        // follow Prev pointers if not in top xref
        //  - make sure no infinite loop in chasing Prev's
        /* ... */
        xref[oi] = offset;
        return xref[oi];
      }
    }
  \end{myc}
  \caption{\dsp{}, A Single Stage, Imperative Parser.}
  \label{fig:dsp}
\end{figure}
%%%%%%%%%%%%%%%

One might ask if it's possible to write a single stage version of the
specification.  I.e., Can we do without so many stages?
The answer is \emph{Yes, but at a cost we don't want to pay!}
Let's refer to the our multi-stage specification as the \ssp{}
(staged) specification.
%
We will compare it to a single-stage specification\footnote{
  Which will not be so clear, but we see no way around this.
}, the \dsp{}
(dynamic) specification in which stage 1 is the same
but stages 2, 3, and 4 are
merged together.
%
In our experience, PDF tools generally have structure more similar to \dsp{} than \ssp{}.

\dsp must be understood imperatively \todo{this seems important: why?};
%
see \cref{fig:dsp} for a sketch
of a possible implementation in C-like notation.
The key things to note about \dsp{} are
\begin{itemize}
\item \lstcd{XREF_ENTRY} and \lstcd{DOM_ENTRY} are both types that mutate progressively
   from unparsed/unknown to fully evaluated.
\item \lstcd{deref()} and \lstcd{parseObject()} are mutually recursive functions.
\item implementing \lstcd{infiniteLoopDetected()} is non-trivial.
\end{itemize}
Also to note about \dsp{}:
\begin{itemize}
\item it is \emph{not} equivalent to \ssp{}: \dsp{} potentially reads
  less of the input file and accepts more input files.  it is
  naturally ``lax'': it would for instance allow a \lstcd{Length} to
  be stored in an \lstcd{ObjStm} stream as long as an infinite loop
  wasn't detected.  It would be possible to extend (and complicate) \dsp{}
  to approximate \ssp{} better.
  % E.g.,
  %  - have a =derefLength= / =derefFromUncompressed=
  %  - More complicated than just this, because this won't catch error if we
  %    luck out and when we request the length it is already decoded.
\item it is nicely lazy if the PDF tool doesn't need to \lstcd{deref}
  every object identifiers, even more lazy than \ssp{}.
\end{itemize}


\ssp{} appears to be strongly preferable to \dsp for two main reasons:
\begin{itemize}
\item It corresponds to the standard, and does so obviously.
\item It does not use general recursion, so it is obvious that \ssp{}
  algorithm terminates on all inputs.
\end{itemize}
Compared to \dsp{}, \ssp{} holds the following advantages:
\begin{enumerate}
\item The functional, declarative, and typed structure enables
  us to understand the stages conceptually.  (Even if one chooses
  not to implement in a like manner.)
\item It demonstrates the non obvious fact that one can
  implement stages 2-4 and keep the ``state of evaluation'' of each
  object identifier the same in each pass.
\item We know exactly what is and isn't parsed, regardless of the
  order in which one traverses the \lstcd{xref}.
\item It is intrisically parallelizable due to the extensive use of
  map-like combinators.
\item The declarative nature of \ssp{} makes it very amenable to
  modification: e.g., the simple addition of the \lstcd{validate}
  operator.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Incremental Updates and Signatures: What A Tangled Web}
\label{sec:updates-and-signatures}

PDF supports the ability to use digital signatures to sign documents
as part of the PDF standard itself.  Signed documents can
subsequently be signed or ``incrementally updated,'' repeatedly.

Digital signatures are just a special form of incremental update, and
thus any PDF reader can just ignore the signature and process it as a
trivial update.  This allows readers that don't support signatures to
elegantly ignore them.

However, when we \emph{do} have a digital signatures and a supporting
reader, things get complicated, as evidenced by Shadow Attacks
\cite{mladenovTrillionDollarRefund2019,ndsssymposiumNDSS2021Shadow2021}.
%
To validate a PDF with a digital signature entails identifying at
which iteration of the PDF document the digital signature was applied
and then validating the digital signature in the context of that
specific DOM and the objects that were in effect at that instance in
time.

But this method completely \emph{breaks} the incremental update
abstraction.  As we discussed, stages 2 and 3 handle all
the incremental update processing and then the information is gone as
it is unneeded by subsequent stages.
% 
This was a reasonable design until ``incremental updates'' were
inadvisedly repurposed for signatures.  Now an incremental update has
semantic content, and a reader supporting validation and display of
signed PDFs will need to keep track of the contents of the DOM at each
signature.

Our specification does \emph{not} support signature validation or
display, and it would be a non-trivial effort to do this.  However,
this complication is going to be reasonable with our \ssp{} approach;
whereas it is unclear whether the \dsp{} approach is able to support
this at all.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis of results}
\label{sec:results-analysis}

% claim conformance to the spec:
The specification as presented adheres to the PDF 2.0 standard.
% bugs found in the standard:
In the process of producing the format definition described above, we
raised multiple issues concerning the PDF standard with the
ISO~\cite{isotc171sc2wg8ISO32000220202020}, specifically an issues
concerning the definition of cross-reference streams.
%
In the process, we raised several other issues related to the
definition of cross-reference streams that were acknowledged by PDFA
and have since been resolved, including %
\textbf{(1)} requirements for object numbers in cross-reference
subsections~\cite{pdfIssue146} and %
\textbf{(2)} corrections to the denotation of the \texttt{XRefStm}
field of a trailer dictionary~\cite{pdfIssue147}.

\todo{Peter: confirm that these issues are related to the paper}

% handling de-facto specs:
The specification's conformance to the standard arguably
limits its applicability given that many PDF tools allow multiple
variances from the standard.
%
However, the opportunity for us is to use our specification to encode
some of these common extensions and to explore if the combination of
these extensions are truly unambiguous.

With our specification based approach, it would not be onerous
to refactor our specification to support different PDF variations:
\begin{itemize}
\item strict PDF-2.0.
\item strict PDF-2.0, validating everything.
\item PDF-2.0 with some common extensions that are determined to be unambiguous.
\end{itemize}
%
\todo{what is it about the spec that would make this easy?}

% discuss concision:
Our specification owes its concision to multiple factors:
\begin{enumerate}
\item our intention to make it as clear as possible,
\item ignoring some ``engineering'' aspects (e.g., informative error
   messages, recovery, etc.), and
\item omitting code for some of the simple, tedious functions.
\end{enumerate}
The current specification could be extended to form a complete
\emph{reference implementation} by extending it to include
%
\textbf{(1)} implementations of relatively minor features that were
not the focus of the current work;
%
\textbf{(2)} implementations of all parsers referenced as primitives; and
%
\textbf{(3)} computation that generates the document's DOM.

% future work:
Future work might also involve adding further validation checks,
including %
\textbf{(1)} processing linearization data and ensuring consistency
with the non-linearized DOM; %
\textbf{(2)} processing both ``branches'' of a hybrid PDF and ensuring
some form of constency between pre 1.5 readers and 1.5+ readers; %
\textbf{(3)} adding signature validation, see
\cref{sec:updates-and-signatures}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% not implemented functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% begin: Hs code not in paper %%%%
\iffalse
\begin{code}

trailersConsistentAcrossUpdates :: [Update] -> Bool
trailersConsistentAcrossUpdates = stub

-- signatures above:
removeFrees m = M.mapMaybe
                  (\x -> case x of Right y -> Just y
                                   _       -> Nothing)
                  m
thawXRefEntry = notImplementedYet
getObjIds = notImplementedYet

\end{code}
\fi
%%%% end: Hs code not in paper %%%%

