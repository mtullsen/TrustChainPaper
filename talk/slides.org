* Settings                                                         :noexport:

#+TITLE: Strengthening Weak Links in the PDF Trust Chain
#+AUTHOR: Mark Tullsen, William Harris, Peter Wyatt
#+Email: tullsen@galois.com, wrharris@galois.com, peter.wyatt@pdfa.org

#+LaTeX_CLASS: beamer
% #+LATEX_CLASS_OPTIONS: [presentation,t]
% #+LATEX_CLASS_OPTIONS: [presentation,10pt]
% #+LATEX_CLASS_OPTIONS: [draft]
#+LATEX_CLASS_OPTIONS: [t,10pt,xcolor={dvipsnames}]
#+BEAMER_THEME: Madrid
#+BEAMER_FRAME_LEVEL: 2

#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)
#+OPTIONS: with-todo-keywords:t
#+OPTIONS:   H:2 num:t toc:t \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:nil pri:nil tags:nil
#+OPTIONS:   author:t inline:t

#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
% #+STARTUP: fninline

#+LATEX_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{}\tableofcontents[currentsection]\end{frame}}
#+LATEX_HEADER: \definecolor{Orange}{rgb}{1,0.5,0}
%  #+LATEX_HEADER: \include{prelude-slides}
#+LATEX_HEADER: \usepackage{listings}

% having no luck: !
% #+LATEX_HEADER: \usepackage{pgfpages}
% #+LATEX_HEADER: \setbeameroption{show notes}
% #+LATEX_HEADER: \setbeameroption{hide notes} % Only slides
% #+LATEX_HEADER: \setbeameroption{show only notes} % Only notes
% #+LATEX_HEADER: \setbeameroption{show notes on second screen=left} % Both

* TODO items/meta                                                  :noexport:

- NOTE
  - 10 mins (Research reports: the total is 15 mins including Q&A)
  - around 10 slides!

- determine what's in/out  
  - parser/validator slides
    - BTW: the standard is effectively defining a validator
      - no guidance as to how to write a robust parser
  - [x] stage 4 Explanation.
  - [?] shotgun and APIs! 
  - [-] haskell code/types? no

- orphans/say
  - with daedalus ddl: spoiled, but you have *lots* of computation!   
  - our paper describes
    - an efficient and purely functional approach
      
# A     
- [ ] code listings: stage 4 and elsewhere
- [ ] bring in text/notes from other instantiations of talk
 
# B 
- [ ] emails on title page
- [ ] spell check
- [ ] practice 3x
  - some you've never given: Stage 4
      
* Pre-DOM: "What Lies Beneath" is a Concern
** DONE PDF Complexity?

#+begin_export latex
\begin{center}
 { \hspace{5pt}
   \includegraphics[width=0.4\linewidth]{../figures/pdf-structure.png}
 } \hspace{30pt}
 \raisebox{-1\baselineskip}
          {\includegraphics[width=0.30\linewidth]{../figures/pdf-structure-incremental.png}}
\end{center}
#+end_export

** DONE PDF Trust Chain

#+begin_export latex
\begin{center}
\includegraphics[width=0.47\linewidth]{../figures/Stages.png}
\end{center}
#+end_export

** DONE Vulnerabilities Occurring Primarily Pre-DOM

- Schizophrenic files (different tools, different renderings)
- Polyglot files (file being in 2+ formats)
- Shadow attacks
  - i.e., attacker can add "shadow content" that is PDF-signed, then reveal
    at will without giving clear warnings to user.
# FIXME    
  - possible because of ability to sign *dead objects* and *cavities*
- Multiple places for hidden/unused/malicious data in PDF
  - non-obvious places, unnoticed when "simply parsing"
  - e.g., shadow-attacks
  - dead bytes, dead objects, dead updates, dead linearization sections, etc.

** DONE PDF, and Pre-DOM, Challenges

- Lack of formality in standard. Thus, implementations:
  - are more effort
  - over implement, under implement, wrongly implement
- No definition of acceptable, reasonable error recovery
- Less than ideal design that reflects 27 years of an evolving standard
- Pre-DOM processing
  - is where many parsing errors & recovery occur
  - is non-trivial
  - involves multiple interdependent features
  - involves multiple redundant features
    - schizophrenic if these features aren't mutually consistent
      
* Modeling Pre-DOM: Highlights
** DONE Stage 4: Transform XRef Map to Object Map

#+begin_export latex
\begin{center}
\includegraphics[width=0.8\linewidth]{images/diagram1/cropped-diagram1.001.png}
\end{center}
#+end_export
#+begin_src bash
      ...
100   3 0 obj 99 endobj
123   % object 4 is not here
151   5 0 obj
      <<
      /Type /ObjStm
      /Length 3 0 R   % indirect!
      /N 2            % 2 objects; (potentially indirect)
      /First 10       % offset to 1st object (potentially indirect)
      >>
      stream
      4 0 6 100
      V1 % PDF-Value here, 4 0 R, [fake comment] 
      V2 % PDF-Value here, 6 0 R, [fake comment]
      endstream
      endobj
409   7 0 obj ... endobj
      ...
#+end_src

** TODO [#C] Specification Issue [Update the above]               :noexport:
** DONE Parser NEQ Validator

Validator: only valid PDFs can produce DOM (must Fail otherwise)

#+begin_export latex
\vspace{10pt}
\includegraphics[width=0.90\linewidth]{images/pNEQv-1.png}
#+end_export


** DONE Parser NEQ Validator

Parser: efficiently, construct the correct DOM when a valid PDF
#+begin_export latex
\vspace{10pt}
\includegraphics[width=0.90\linewidth]{images/pNEQv-2.png}
#+end_export

** TODO [#C] Parser NEQ Validator

#+begin_export latex
\vspace{20pt}
\includegraphics[width=0.90\linewidth]{images/pNEQv-3.png}
#+end_export

** DONE Turning Parser into Validator

Parser specification is designed to be
- understandable: clear, pure Haskell
- phased, clearly terminating (get parallelizability for free) 
- very lazy "Parser" (big input cloud)
  
We can extend spec into a validator, orthogonally, via “validate” constructs
(turning on/off on with command-line flag).  E.g.,
#+begin_src haskell
do
  (xrefRaw, xrefEndOff) <- pXrefRaw
  validate $
     verifyXrefRaw xrefRaw -- ensures no duplicates
#+end_src

* Looking Forward
** TODO [#A] Accomplishments                                          

- A specification for pre-DOM parsing/computation
  - Clarifies some subtle issues in PDF Standard
  - A growing list of PDF Association “issues” that we have contributed to
    creating [23,24,...,30]
- Have a unique tool for displaying updates & cavities
- A semantics that ... [FIXME]
  
** TODO Future [FIXME: TOO MUCH]

- Expand spec (uninteresting parts are unimplemented)
- Turn spec into reference implementation (of pre-DOM)
  - integrate with primitive parsers (implemented of course with Daedalus)
  - Thus, can test other tools’ conformance with our reference implementation
- Extend specification
  - support more PDF features (hybrids, compression, …)
  - add support for commonly allowed “exuberances”
  - add more “validate”s to get closer to a Validator
- Our "pre-DOM inspection/validation tool"
  - update semantics (as learned from writing spec)
  - further extensions

** DONE Implementation?

*[TODO: in the paper we note that we have, from scratch,
specified the hard parts but have not linked to our [daedalus generated]
primitive parsers]*

Tools & renderers rarely need (/demand/) the whole PDF
 - reading?
 - parsing??
 - semantic checks???
#+latex: \vspace{12pt}
   
Thus, this
#+begin_src haskell
  parsePDF :: FileData -> Maybe PDFAbstractSyntax
#+end_src
is not going to be used in practice!     

# Alternatives?

** DONE One Solution ...

- For complex formats,
  - tools are "projections": rarely to parse/validate all.
  - may have alternate "parsing paths" we want to take

- Shotgun Parsers?
  - ... the deadliest of patterns: "Input data checking, handling interspersed
    with processing logic"
- I.e., we provide multiple parsers where the following is interspersed through
  code and the relation between these is *not specified*:
  #+begin_src haskell
    parseA :: Offset -> IO A
    parseB :: Offset -> IO B
    parseC :: Offset -> IO C
    validateA :: A -> IO ()
    validateB :: A -> B -> IO ()
  #+end_src

** DONE Better Solution, Parser as API

We provide four interdependent calls (not /entry points/):
#+begin_src haskell
  parseHdrTrlr :: FileData -> IO HdrTrlr
  parseUpdates :: HdrTrlr -> IO [Updates]
  createXRef   :: [Updates] -> IO XRef
  derefObjId   :: ObjId -> XRef -> IO PdfValue
#+end_src
(Types can be as abstract as we wish.)

#+latex: \vspace{18pt}
Using this, we write abstractions on the above:
#+begin_src haskell
  getInitialUpdate :: FileData -> IO XRef
  getRootValue     :: HdrTrailer -> XRef -> PdfValue
  getPageTree      :: XRef -> Tree PdfValue
#+end_src

# https://darkbazaar.wordpress.com/category/researchers/bratus-sergey/
# 
#   Sadly, a lot of actual input handling code is a mixture of data processing
#   and recognition, scattered throughout a codebase. Its “sanity checking” is
#   neither strong enough to verify all the implicit assumptions, nor written
#   with these assumptions in mind. We call such input handling code “shotgun
#   parsers” and argue that it’s the number 1 reason for the ubiquitous
#   insecurity of programs facing the internet.

