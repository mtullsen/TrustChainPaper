* Settings                                                         :noexport:

#+TITLE: Strengthening Weak Links in the PDF Trust Chain
#+AUTHOR: Mark Tullsen, William Harris, Peter Wyatt
#+Email:  tullsen@galois.com, wrharris@galois.com, peter.wyatt@pdfa.org
#+date:   May 26, 2022

#+LaTeX_CLASS: beamer
% #+LATEX_CLASS_OPTIONS: [presentation,t]
% #+LATEX_CLASS_OPTIONS: [presentation,10pt]
% #+LATEX_CLASS_OPTIONS: [draft]
#+LATEX_CLASS_OPTIONS: [t,10pt,xcolor={dvipsnames}]
#+BEAMER_THEME: Madrid
#+BEAMER_FRAME_LEVEL: 2

#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)
#+OPTIONS: with-todo-keywords:t
#+OPTIONS:   H:2 num:t toc:t \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:nil pri:nil tags:nil
#+OPTIONS:   author:f inline:t

#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
% #+STARTUP: fninline

#+LATEX_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{}\tableofcontents[currentsection]\end{frame}}
#+LATEX_HEADER: \definecolor{Orange}{rgb}{1,0.5,0}
%  #+LATEX_HEADER: \include{prelude-slides}
#+LATEX_HEADER: \usepackage{listings}
  % no-op at the moment
  
% having no luck: !
% #+LATEX_HEADER: \usepackage{pgfpages}
% #+LATEX_HEADER: \setbeameroption{show notes}
% #+LATEX_HEADER: \setbeameroption{hide notes} % Only slides
% #+LATEX_HEADER: \setbeameroption{show only notes} % Only notes
% #+LATEX_HEADER: \setbeameroption{show notes on second screen=left} % Both

#+LATEX_HEADER: \author{Mark Tullsen, William Harris, Peter Wyatt \\ \\ {tullsen,wrharris}@galois.com \\ peter.wyatt@pdfa.org }

* TODO items/meta                                                  :noexport:

- NOTE
  - 10 mins (Research reports: the total is 15 mins including Q&A)
  - around 10 slides!

- determine what's in/out  
  - parser/validator slides
    - BTW: the standard is effectively defining a validator
      - no guidance as to how to write a robust parser

- orphans/say
  - with daedalus ddl: spoiled, but you have *lots* of computation!   
  - our paper describes
    - an efficient and purely functional approach

- POST-PROCESS
  - fixup author
          
# A
- [ ] lost your pre-DOM notes!! p5.
- [ ] talk through your Stage4 slide
- [ ] practice 3x
  - some you've never given: Stage 4
- [ ] code listings: indent, and make stage 4 smaller/two wide.

# B 
- [ ] spell check
- [ ] bring in text/notes from other instantiations of talk
      
* Pre-DOM (Pre Document Object Model)
** DONE PDF Complexity?

#+begin_export latex
\begin{center}
 { \hspace{5pt}
   \includegraphics[width=0.4\linewidth]{../figures/pdf-structure.png}
 } \hspace{30pt}
 \raisebox{-1\baselineskip}
          {\includegraphics[width=0.30\linewidth]{../figures/pdf-structure-incremental.png}}
\end{center}
#+end_export

** TODO PDF Trust Chain: Pre-DOM and Post-DOM

#+begin_export latex
\begin{center}
\includegraphics[width=0.63\linewidth]{images/trustchain-with-braces.png}
\end{center}
#+end_export

# - Pre-DOM = ...
#     - stage 5 & 6: quite complex
#     - stages 1-4: very not trivial
#       - and can undermine everything subsequent
#         - which is done in practice!
           
** DONE Vulnerabilities Occurring Primarily Pre-DOM

- Schizophrenic files (different tools, different renderings)
- Polyglot files (file being in 2+ formats)
- Shadow attacks
  - i.e., attacker can
    - add "shadow content" that is PDF-signed,
    - after signing, can update-at-will (revealing shadow content)
    - without giving clear warnings to user.
  - possible because of ability to sign /dead objects/ and /cavities/
- Multiple places for hidden/unused/malicious data in PDF
  - non-obvious places, unnoticed when "simply parsing"
  - e.g., shadow-attacks
  - dead bytes, dead objects, dead updates, dead linearization sections, etc.

# FIXME: describing shadow-attacks
    
** DONE [#C] PDF, and Pre-DOM, Challenges

- Lack of formality in standard. Thus, implementations:
  - are more effort
  - over implement, under implement, wrongly implement
- No definition of acceptable, reasonable error recovery
- Less than ideal design that reflects 27 years of an evolving standard
- Pre-DOM processing
  - is where many parsing errors & recovery occur
  - is non-trivial
  - involves multiple interdependent features
  - involves multiple redundant features
    - schizophrenic if these features aren't mutually consistent
      
* Modeling Pre-DOM: Highlights
** DONE Stage 4 Sub-Stages: Transform XRef Map to Object Map

#+begin_export latex
\begin{center}
\includegraphics[width=0.8\linewidth]{images/diagram1/cropped-diagram1.001.png}
\end{center}
#+end_export
#+begin_src
      ...
100   3 0 obj 99 endobj
123   % object 4 is not here
151   5 0 obj
      <<
      /Type /ObjStm
      /Length 3 0 R   % indirect!
      /N 2            % 2 objects; (potentially indirect)
      /First 10       % offset to 1st object (potentially indirect)
      >>
      stream
      4 0 6 100
      V1 % PDF-Value here, 4 0 R, [fake comment] 
      V2 % PDF-Value here, 6 0 R, [fake comment]
      endstream
      endobj
409   7 0 obj ... endobj
      ...
#+end_src

** TODO [#C] Specification Issue [Update the above]               :noexport:
** DONE Parser $\neq$ Validator
- A surprising source of mis-communication.
- ...
** DONE A Validator (Parser $\neq$ Validator)

#+begin_export latex
\vspace{10pt}
\includegraphics[width=0.80\linewidth]{images/pNEQv-1.png}
\vfill
#+end_export

VALIDATOR: only valid PDFs can produce DOM (must Fail otherwise)

** DONE A Parser (Parser $\neq$ Validator)

#+begin_export latex
\vspace{10pt}
\includegraphics[width=0.80\linewidth]{images/pNEQv-2.png}
\vfill
#+end_export

PARSER: efficiently, construct the correct DOM when a valid PDF

** DONE A Very Accepting Parser (Parser $\neq$ Validator)

#+begin_export latex
\vspace{10pt}
\includegraphics[width=0.95\linewidth]{images/pNEQv-3.png}
#+end_export

** DONE Turning Parser into Validator

Parser specification is designed to be
- understandable: clear, pure Haskell
- phased, clearly terminating (get parallelizability for free) 
- very lazy "Parser" (big input cloud)
  
We can extend spec into a validator, orthogonally, via “validate” constructs
(turning on/off on with command-line flag).  E.g.,
#+begin_src haskell
  do
  (xrefRaw, xrefEndOff) <- pXrefRaw
  validate $
     verifyXrefRaw xrefRaw -- ensures no duplicates
#+end_src

* Conclusions
** DONE Accomplishments                                          

- A specification for pre-DOM parsing/computation
  - Clarifies some subtle issues in PDF Standard
  - A growing list of PDF Association “issues” that we have contributed to
    creating [23,24,...,30]
- Cause /and/ effect of
  - unique tool for displaying updates & cavities
      
** DONE Future

- Not accomplished yet
  - the less interesting/subtle parts specified/implemented 
  - integrated with our primitive, daedalus-generated parsers to create
    a  tool.

- Create a full pre-DOM tool that
  - supports further PDF features (hybrids, compression, …)
  - add support for commonly allowed “exuberances”
  - add more “validate”s to get closer to a /validator/.

* Preview: A Next Step
** DONE Implementation?

Tools & renderers rarely need (/demand/) the whole PDF
 - reading?
 - parsing??
 - semantic checks???
#+latex: \vspace{12pt}
   
Thus, this
#+begin_src haskell
  parsePDF :: FileData -> Maybe PDFAbstractSyntax
#+end_src
is not going to be used in practice!     

# Alternatives?

** DONE One Solution ...

- For complex formats,
  - tools are "projections": rarely used parse/validate all.
  - may have alternate "parsing paths" we want to take
    - e.g., metadata, page 1, text-only

- Shotgun Parsers?
  - ... the deadliest of patterns: "Input data checking, handling interspersed
    with processing logic"

- I.e., we provide multiple parsers where the following is interspersed through
  code and the relation between these is *not specified*:
  #+begin_src haskell
    parseA :: Offset -> IO A
    parseB :: Offset -> IO B
    parseC :: Offset -> IO C
    validateA :: A -> IO ()
    validateB :: A -> B -> IO ()
  #+end_src

** DONE Better Solution, Parser as API

We provide four inter-dependent calls (not /entry points/):
#+begin_src haskell
  parseHdrTrlr :: FileData -> IO HdrTrlr
  parseUpdates :: HdrTrlr -> IO [Updates]
  createXRef   :: [Updates] -> IO XRef
  derefObjId   :: ObjId -> XRef -> IO PdfValue
#+end_src
(The returned types can be as abstract as we wish.)

#+latex: \vspace{18pt}

Using this, we write abstractions on the above:
#+begin_src haskell
  getInitialUpdate :: FileData -> IO XRef
  getRootValue     :: HdrTrailer -> XRef -> PdfValue
  getPageTree      :: XRef -> Tree PdfValue
#+end_src

# https://darkbazaar.wordpress.com/category/researchers/bratus-sergey/
# 
#   Sadly, a lot of actual input handling code is a mixture of data processing
#   and recognition, scattered throughout a codebase. Its “sanity checking” is
#   neither strong enough to verify all the implicit assumptions, nor written
#   with these assumptions in mind. We call such input handling code “shotgun
#   parsers” and argue that it’s the number 1 reason for the ubiquitous
#   insecurity of programs facing the internet.

